{"Blog/Building-an-object-recognizer-iOS-app-using-CoreML,-Vision-and-Swift":{"title":"Building an object recognizer iOS app using CoreML, Vision and Swift","links":["tags/blog/post"],"tags":["blog/post"],"content":"post\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe CoreML and Vision frameworks were amongst some of the coolest new tech announced at WWDC on Wednesday (7 Jun).\nCoreML makes it really easy to integrate pre-trained machine learning models into your iOS app using either Swift or Objective C. It abstracts out various details of how the model works and lets the developer focus on just the code. CoreML even decides whether to run your model on the device’s CPU (typically for memory heavy models), or on the GPU (typically for compute heavy models) and exposes a set of easy to use developer APIs. At the time of writing, Apple also provides a tool to easily convert models built using Caffe, Keras and Scikit-Learn into a .mlmodel file that can be consumed in Xcode. Some supported model types include neural networks (feedforward, convolutional and recurrent), tree ensembles, support vector machines and liner models including linear and logistic regression.\nAmongst various other computer vision related tasks, the Vision API also plays nice with CoreML and makes it easy to feed it visual data.\nI have limited Swift experience, but couldn’t wait to try out the CoreML framework. I decided to try building an image classifier, similar to what was demoed at WWDC. The app should take frames from the device’s camera and output top predictions of what the camera is pointed at. This blog explains how you can create such an app.\nRequirements\niOS 11 Beta (Get this straight from Apple if you’re enrolled in the paid developer program, or download for free from beta.applebetas.co)\nXcode 9 Beta\nUI and Camera setup\nWe’ll start by simply displaying the camera output on screen, and then adding the machine learning part to it. Let’s go ahead and create a single-view iOS application in Xcode with swift. We want the UI to simply show an area for the camera output, along with a text area below it to show the prediction. Let’s create this by dragging and dropping a UIView and a Label into Main.storyboard and applying the appropriate constraints.\n\nClick on the label field, go to the attributes inspector on the right sidebar, and change ‘Lines’ from 1 to 4. We’ll need this in order to display multiple predictions returned by the model. Let’s also connect our UIView and Label to ViewController.swift by Ctrl + dragging them into the file.\n\nIn order to be able to obtain frames from the camera, iOS requires you to add a ‘Privacy — Camera Usage Description’ key in Info.plist. You can enter any value for this key that describes why your app requires camera access. In ViewController.swift let’s now import AVFoundation, implement the AVCaptureVideoDataOutputSampleBufferDelegate protocol and add code to fetch camera frames and display them on our cameraView UIView. Your code should look something like this —\nimport UIKit\nimport AVFoundation\n \nclass ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate {\n    // Connect InterfaceBuilder views to code\n    @IBOutlet weak var classificationText: UILabel!\n    @IBOutlet weak var cameraView: UIView!\n \n    // Create a layer to display camera frames in the UIView\n    private lazy var cameraLayer: AVCaptureVideoPreviewLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)\n    // Create an AVCaptureSession\n    private lazy var captureSession: AVCaptureSession = {\n        let session = AVCaptureSession()\n        session.sessionPreset = AVCaptureSession.Preset.photo\n        guard\n            let backCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),\n            let input = try? AVCaptureDeviceInput(device: backCamera)\n            else { return session }\n        session.addInput(input)\n        return session\n    }()\n \n    override func viewDidLoad() {\n        super.viewDidLoad()\n        self.cameraView(self.cameraLayer)\n        let videoOutput = AVCaptureVideoDataOutput()\n        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: &quot;MyQueue&quot;))\n        self.captureSession.addOutput(videoOutput)\n        self.captureSession.startRunning()\n    }\n \n    override func viewDidLayoutSubviews() {\n        super.viewDidLayoutSubviews()\n        self.cameraLayer.frame = self.cameraView ?? .zero\n    }\n \n    override func didReceiveMemoryWarning() {\n        super.didReceiveMemoryWarning()\n        // Dispose of any resources that can be recreated.\n    }\n}\n \nRunning your app on device should now show camera frames on screen.\nMachine Learning\nIt’s now time to think about how to integrate an object detection machine learning model into our app. Let’s use an Inception V3 pre-trained model available online. A copy can be downloaded directly from apple (Download CoreML Model). Alternatively, you can use your own model and convert it to a .mlmodel file using a python tool provided by apple.\nOnce you have the model file, simply drag and drop the file into Xcode. Clicking on the model file should show you information on the shapes of inputs and outputs. In this case, you’ll see that the input is an image of size 299 x 299; the output is a dictionary where the key is a string describing the object found and the value is a probability describing how confident the prediction is.\n\nLet’s now import the Vision and CoreML APIs and and declare an array of VNRequests on the ViewController class. This array will hold any machine learning models we want to run on each image. In our case, we’ll just have the 1 model that classifies images. Let us also define a setupVision() function to set up our computer vision tasks, and a handleClassifications function to act as the completion handler for our VNCoreMLRequest. We’ll wire everything up by implementing the captureOutput(_:didOutput:from:) method in the AVCaptureVideoDataOutputSampleBufferDelegate specification. This notifies the delegate every time a new video frame is received.\n// ViewController.swift\nprivate var requests = [VNRequest]()\n \noverride func viewDidLoad() {\n    // Same as before\n    setupVision()\n}\n \nfunc setupVision() {\n    guard let visionModel = try? VNCoreMLModel(for: Inceptionv3().model)\n        else { fatalError(&quot;Can&#039;t load VisionML model&quot;) }\n    let classificationRequest = VNCoreMLRequest(model: visionModel, completionHandler: handleClassifications)\n    classificationRequest.imageCropAndScaleOption = VNImageCropAndScaleOptionCenterCrop\n    self.requests = [classificationRequest]\n}\n \nfunc handleClassifications(request: VNRequest, error: Error?) {\n    guard let observations = request.results\n        else { print(&quot;no results: \\(error!)&quot;); return }\n    let classifications = observations[0...4]\n        .flatMap({ $0 as? VNClassificationObservation })\n        .filter({ $0.confidence &gt; 0.3 })\n        .map {\n            (prediction: VNClassificationObservation) -&gt; String in\n            return &quot;\\(round(prediction.confidence * 100 * 100)/100)%: \\(prediction.identifier)&quot;\n        }\n    DispatchQueue.main.async {\n        print(classifications.joined(separator: &quot;###&quot;))\n        self.classificationText.text = classifications.joined(separator: &quot;\\n&quot;)\n    }\n}\n \nfunc captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {\n        return\n    }\n    var requestOptions:[VNImageOption : Any] = [:]\n    if let cameraIntrinsicData = CMGetAttachment(sampleBuffer, kCMSampleBufferAttachmentKey_CameraIntrinsicMatrix, nil) {\n        requestOptions = [.cameraIntrinsics:cameraIntrinsicData]\n    }\n    let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: 1, options: requestOptions)\n    do {\n        try imageRequestHandler.perform(self.requests)\n    } catch {\n        print(error)\n    }\n}\n \nYour entire ViewController.swift should look something like this:\n// ViewController.swift\nimport UIKit\nimport AVFoundation\nimport CoreML\nimport Vision\n \nclass ViewController: UIViewController, AVCaptureVideoDataOutputSampleBufferDelegate {\n    // Connect UI to code\n    @IBOutlet weak var classificationText: UILabel!\n    @IBOutlet weak var cameraView: UIView!\n \n    private var requests = [VNRequest]()\n    private lazy var cameraLayer: AVCaptureVideoPreviewLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)\n    private lazy var captureSession: AVCaptureSession = {\n        let session = AVCaptureSession()\n        session.sessionPreset = AVCaptureSession.Preset.photo\n        guard\n            let backCamera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),\n            let input = try? AVCaptureDeviceInput(device: backCamera)\n            else { return session }\n        session.addInput(input)\n        return session\n    }()\n \n    override func viewDidLoad() {\n        super.viewDidLoad()\n        self.cameraView(self.cameraLayer)\n        let videoOutput = AVCaptureVideoDataOutput()\n        videoOutput.setSampleBufferDelegate(self, queue: DispatchQueue(label: &quot;MyQueue&quot;))\n        self.captureSession.addOutput(videoOutput)\n        self.captureSession.startRunning()\n        setupVision()\n    }\n \n    override func viewDidLayoutSubviews() {\n        super.viewDidLayoutSubviews()\n        self.cameraLayer.frame = self.cameraView ?? .zero\n    }\n \n    func setupVision() {\n        guard let visionModel = try? VNCoreMLModel(for: Inceptionv3().model)\n            else { fatalError(&quot;Can&#039;t load VisionML model&quot;) }\n        let classificationRequest = VNCoreMLRequest(model: visionModel, completionHandler: handleClassifications)\n        classificationRequest.imageCropAndScaleOption = VNImageCropAndScaleOptionCenterCrop\n        self.requests = [classificationRequest]\n    }\n \n    func handleClassifications(request: VNRequest, error: Error?) {\n        guard let observations = request.results\n            else { print(&quot;no results: \\(error!)&quot;); return }\n        let classifications = observations[0...4]\n            .flatMap({ $0 as? VNClassificationObservation })\n            .filter({ $0.confidence &gt; 0.3 })\n            .sorted(by: { $0.confidence &gt; $1.confidence })\n            .map {\n                (prediction: VNClassificationObservation) -&gt; String in\n                return &quot;\\(round(prediction.confidence * 100 * 100)/100)%: \\(prediction.identifier)&quot;\n            }\n        DispatchQueue.main.async {\n            print(classifications.joined(separator: &quot;###&quot;))\n            self.classificationText.text = classifications.joined(separator: &quot;\\n&quot;)\n        }\n    }\n \n    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else {\n            return\n        }\n        var requestOptions:[VNImageOption : Any] = [:]\n        if let cameraIntrinsicData = CMGetAttachment(sampleBuffer, kCMSampleBufferAttachmentKey_CameraIntrinsicMatrix, nil) {\n            requestOptions = [.cameraIntrinsics:cameraIntrinsicData]\n        }\n        let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: 1, options: requestOptions)\n        do {\n            try imageRequestHandler.perform(self.requests)\n        } catch {\n            print(error)\n        }\n    }\n \n    override func didReceiveMemoryWarning() {\n        super.didReceiveMemoryWarning()\n    }\n}\nAt this point your app should be recognizing objects its pointed at!\nAll code for the project can be found at github.com/G2Jose/ObjectClassifier\nReferences\n\nWWDC 2017\nVision Framework\nCoreML Framework\n"},"Blog/CodePull---An-internal-tool-to-democratize-feature-testing-at-Drop":{"title":"CodePull - An internal tool to democratize feature testing at Drop","links":["tags/blog/post","tags/react-native","tags/typescript","tags/developer-experience"],"tags":["blog/post","react-native","typescript","developer-experience"],"content":"post react-native typescript developer-experience\n\n\n                  \n                  Note\n                  \n                \n\nThis is a shorter version of a more detailed post I made internally at Drop. Some details have been omitted here to preserve confidentiality.\nAfter its initial adoption in June 2021, CodePull became a core part of the development and testing workflow at Drop.\n\n\nBackground\nReact native is used at Drop to power the consumer app. React Native allows code to be written in Typescript, which under the hood calls platform specific APIs, resulting in the same codebase powering a fully native experience on Android &amp; iOS.\nAt a very high level, the overall development process followed currently looks something like this:\n\nEngineer writes code for a new feature on a branch, based on specs &amp; designs aligned on as a cross functional team (engineering, design, product, data and other relevant functions for the feature in question)\nOnce ready, the engineer creates a Pull Request to be reviewed by their peers.\n\nUp until now, the engineer can really only see the changes they’re making on a simulator, or on a test devices they have.\nTo get more people involved in testing their work up to this point, there 2 ways, broadly speaking:\n\nThe engineer can set up a zoom call with relevant folks (usually design at this point) to walk them through their work via screen sharing\nAlternatively, the engineer could create a new test build and distribute it internally using TestFlight\n\n\n\n\nOnce the code is reviewed and approved by at least 2 other members of the engineering team, the PR is ready to be merged into a release branch\nOnce the code is merged, an automated tool picks it up and pushes the changes to a QA build. From this point, the changes made are widely accessible to cross functional folks, as long as they have access to the QA track.\n\nThe Problem\nLooking at the process above, it becomes apparent that there are a few bottlenecks that can be improved on, particularly in sharing work in progress with cross functional members of the team in a scalable way. In particular:\n\nThe engineer can set up a zoom call with relevant folks (usually design at this point) to walk them through their work\n\nThis does not scale very well, because:\n\nIt is a synchronous process and requires finding a time that works for everyone\nSince the flow is being walked through on a simulator on the engineer’s computer and screen shared, it’s harder to spot small visual issues\nIt’s simply not the same as trying out a flow on one’s device and evaluating how it ‘feels’\n\n\nAlternatively, the engineer could create a new test build and distribute it internally using TestFlight\n\nThis also does not scale very well because:\n\nWe have CI automation in place to automatically push any merged code into QA builds. Manually pushing any other code will get overwritten the next time another engineer’s code is merged\nIt only allows testing one thing at a time. Another engineer trying to do the same will cause their peer’s test deployment to be overwritten\n\n\nOnce the code is merged, an automated tool picks it up and pushes the changes to the QA builds. From this point, the changes made are widely accessible to cross functional folks, as long as they have access to the QA app\n\nTesting at this phase is insufficient, as it can block continuous delivery. Any code merged into a release branch is expected to be free of bugs and ready for release so as to not block other engineers shipping their work.\nWith this in mind, it becomes clear that there is an opportunity to improve tooling to allow sharing work in progress. The ideal solution here would:\n\nMake it quick and easy for an engineer to share their work in progress, without having to merge code into a release branch. There shouldn’t be any additional manual work in doing this.\nMake it quick and easy for the person who wants to test out the work in progress to do so. They should ideally not have to keep track of build numbers, manually update builds etc. The easier the process, the more likely it will be used frequently.\nAny process adopted here should not have any performance or security implications.\n\nThe solution\nIt turns out that we can actually build something to solve this problem. Keeping in mind that the vast majority of our code is written in typescript, with native code changes happening very infrequently, we could solve the above problems by simply making it possible for the app to ‘pull’ compiled typescript code on demand.\nThis isn’t very different to how CodePush works (where we’re able to push production updates to users without going through the App / Play Store).\nWe’re calling this new system CodePull, and this is how it works:\n\nAn engineer commits code and pushes it to GitHub on their own feature branch\nA GitHub action is kicked off in response. The action compiles all the typescript code from the commit.\nIt then pushes the compiled code into an s3 bucket with a special file name based on the git branch\nThe app now has the ability to open special links created based on git branch name (eg: drop://codepull?name=feature-improved-search.zip. The engineer shares the link with anyone on the project team who wants to test.\nOpening the link causes the app to download the relevant WIP compiled code from the s3 bucket and restart itself, running the new code\nOnce done, the individual testing the feature can revert back to the regular app code\n\nLimitations\n\nWhile this should cover us for the majority of code we want to test, there will occasionally be times when we make native changes, where we’ll have to create custom test builds\n"},"Blog/Exploring-Sorbet-with-Rails-at-Drop":{"title":"Exploring Sorbet with Rails at Drop","links":["tags/blog/post","tags/ruby","tags/ruby-on-rails","tags/static-analysis","Blog/Exploring-Sorbet-@-Drop-31390c998fcb49c290a89339944c3af2"],"tags":["blog/post","ruby","ruby-on-rails","static-analysis"],"content":"post ruby ruby-on-rails static-analysis\n\n\n                  \n                  Note\n                  \n                \n\nThis is a shorter version of a more detailed post I made internally at Drop while researching this topic. Certain details have been removed here to protect confidentiality.\nBased on this initial evaluation, Sorbet was adopted into the backend codebase initially on a trial run and more extensively afterwards.\n\n\nWhat is it?\nSorbet is a type checker for ruby built by Stripe. It aims to be for Ruby, what TypeScript is for JavaScript.\nIt’s designed to be adopted gradually on existing code-bases. Sorbet can be used through code editors, where it can provide a more interactive, productive experience through smart auto-completion suggestions, go-to-definition and highlighted type errors.\nIt can also be run through its CLI, and can be integrated into CI to catch errors before they’re merged and deployed.\nWho uses it?\nSome notable companies using Sorbet include:\n\nStripe (where it was originally built)\nShopify\nCoinbase\nKickstarter\n\nComparison to similar tools\nRBS is a new syntax for representing types in ruby 3. While RBS aims to solve the same problem, it has a few notable differences with Sorbet:\n\nWith RBS, types are represented in separate files, as opposed to annotations within the same file that sorbet supports (sorbet supports both annotations in separate .rbi files, and within regular .rb files).\n\nI believe these will be much harder to maintain and keep in sync over time. This is not very different to how one could define types for JavaScript in separate .d.ts files - this wasn’t adopted by the community except for shipping type definitions for libraries written in JavaScript.\nTalk by Matz, where he talks about RBS and some reasons he wants them to be separate files (he just doesn’t seem to like the idea of type annotations)\n\n\nRBS is just a syntax for defining types. There isn’t an official type checking tool that comes with this, but is instead left to the community to build.\n\nThe main tool available here for type checking is steep. It seems like its corresponding editor extension (built for vscode) hasn’t been updated in a long time. From a quick glance it doesn’t seem like there are companies backing this project.\n\n\nFrom some googling around, I couldn’t find any companies using RBS in production.\n\nThis is in stark contrast to Sorbet which is battle tested at places like Stripe and Shopify, both of who have built open source tooling around this.\n\n\nSome more info can be found here:\n\nSorbet’s FAQ that talks about RBS\n\n\n\nBeing a small engineering team at Drop, I think it’s important to bet on the right horse as we likely will not have the resources to build our own tooling around RBS.\nSorbet @ Drop\nCurrent status\nSorbet has been set up on a branch named … sorbet.\nI’ve tried setting up sorbet both with and without tapioca, and it appears that sorbet + sorbet-rails gives us the most coverage out of the box.\nThis branch has also been set up with spoom, a handy tool built at Shopify that makes it easy to track gradual adoption. (this is what tells us that sorbet + sorbet-rails gives us better coverage than sorbet + tapioca)\nSorbet has been able to add types to a good chunk of our code-base without any manual effort. 58% of call sites are typed, and 43% of all files already have a type level of true or higher.\n\n\n                  \n                  Info\n                  \n                \n\nRedacted coverage report images\n\n\nWhile sorbet also supports runtime checks, this has not been set up. Perhaps in the future it might make sense to enable this for QA. The sorbet branch contains no changes to runtime code, and only adds static checks.\nWhat might adopting it at Drop look like?\n\nGet a proof of concept integration on a branch\nGet other engineers to try it out and gather any initial feedback\nMerge &amp; extended canary. While there shouldn’t be any runtime code changes made here, it never hurts to be safe.\nIntegrate into CI, add pre-commit hooks to validate that existing typings aren’t broken.\n\nFile level typing strictness has been set up on all files in a way that makes it so there are 0 type errors on current code.\n\n\nSet up tooling to measure adoption over time\n\nTo do this, the srb cli has support for statsd. It should be possible to hook this up to DataDog, perhaps as a post-merge GitHub action.\n\n\nExperimentation phase\n\nTyping on new files will be optional during this period, allowing engineers to commit code with or without types (the expectation however will be that no existing types are broken). A dedicated sorbet slack channel will be set up to gather any feedback, address any issues or help folks ramp up during this phase.\n\n\nDepending on feedback from the above phase, align on future expectations across Backend Engineering. Examples of expectations we set may be:\n\nAll new code should meet a certain typing level\nAll touched code should meet a certain type level\nTyping levels can only ever be incremented, never decremented\n\n\n\nWhy might we want to adopt it?\nThe main reasons I see to adopt it are centered around the somewhat related themes of Developer Experience (DX), Productivity and Code Quality.\n\nSmart autocomplete suggestions as you type should make coding a more interactive, fun experience.\nSince these suggestions are based on methods, properties etc that actually exist (as opposed to auto-completion suggestions one might see from an editor in an untyped ruby code-base), it should provide an engineer with more confidence that the line of code they wrote will actually run, making it so one can write more code before having to run and test it. This should reduce the number of “write code” → “run code” → “check that it works” cycles one needs to perform, improving productivity.\nThe interactivity it introduces here should also make it easier for new engineers to onboard onto the code-base. Type annotations should serve as machine validated documentation that’s always in sync.\nType errors shown in one’s editor should further quicken the feedback loop between writing code and knowing whether it’ll run. Along with tests, types should bring in an added layer of safety, as realistically not all possible code branches may be covered by a test (note that this is in no way meant to replace or reduce the number of tests, but to work alongside them).\n\nFAQ on concerns one might have\n\nWill this block me from being able to ship things?\nSince sorbet is designed with incremental adoption in mind, it’s not likely that a type error will block anyone from shipping. A difficult to fix type error can always be suppressed by marking the file as typed: false, or using the T.untyped type.\nWill this mean the code-base will now be littered with types annotations?\n**Not really. Sorbet is quite smart with type inference, unlike say, Java. Type annotations will be needed in places like method signatures, but sorbet will infer types in lots of other places.\nSee related info here.\nDoes this mean I don’t have to write as many tests?\nAbsolutely not - type checking and other forms of static analysis are meant to work on top of, not instead of, tests\n\nCrash Course on Sorbet\nType levels\nTo allow for incremental adoption, sorbet supports various type levels on files. In increasing order of strictness they are: ignore → false → true → strict → strong\n\ntyped: ignore files are not even read by sorbet\nAs noted on the link below, if you un-ignore a file you must run  srb rbi hidden-definitions\ntyped: false causes only syntax and constant resolution errors to be reported\nAny file marked typed: true or higher will report type errors\ntyped: strict requires that all methods be explicitly annotated with types\ntyped: strong we likely should not be using this since:\n\n\n\n                  \n                  Warning\n                  \n                \n\nSupport for typed: strong files is minimal, as Sorbet changes regularly and new features often bring new T.untyped intermediate values.\nTo start getting value out of sorbet, files should be marked as typed: true or above.\nSorbet can automatically scan files and suggest strictness on each file based on type errors that already exist through the srb rbi suggest-typed command. (this has already been done on the sorbet branch)\n\n\n                  \n                  Tip\n                  \n                \n\n📖 You can read more about type levels here.\nUnless exceptional circumstances arise, we should aim to only increase our type level for any given file.\nAdding type annotations\nTyping methods\n(Explicit types are required on methods due to how sorbet parallelizes type-checking for speed. Read more here.)\n\nextend T::Sig at the top of the class / module\nAdd sig {params(param_name: ParamType).returns(ReturnType) } above the method\nAdd require &#039;sorbet-runtime-stub&#039; unless defined?(T)\n\nFor example:\nrequire &#039;sorbet-runtime-stub&#039; unless defined?(T)\n \nclass CollectibleInventoryService\n  extend T::Sig\n \n\tsig {params(amount: Integer).returns(T::Boolean)}\n  def enough_available?(amount:)\n\t\tavailable &gt;= amount\n\tend\nend\nNote that the syntax is the same whether parameters are named or positional.\n\n\n                  \n                  Tip\n                  \n                \n\n📖 You can read more about typing methods here.\nTyping non-methods\nOften explicit type annotations are not required for local variables. Instead, their types can be inferred automatically.\nWhen explicit annotations are required though, T.let can be used.\nExample:\ncollectible = T.let(user.collectible, Collectible) # Collectible is the type\n\n\n                  \n                  Tip\n                  \n                \n\n📖 You can read more about typing non-method constructs here.\nOther useful constructs\n\n\nT.nilable represents a union type with nil\nc = T.let(user.collectible, T.nilable(Collectible))\n# c can be of type nil or Collectible \n\n\nT::Array can be used to type arrays. It also supports generics, which can be used to restrict what objects can be contained in an array\nx = T.let([], T::Array[String])\nx.push(1) # This should result in a type error\n\n\nT::Hash can be used in a similar way with generics\nx = T.let({}, T::Hash[Symbol, Integer])\nx[:key] = &#039;string&#039; # should result in an error\n\n\nT.any can be used to represent union types\nT.any(String, Integer)\n\n\nValidating types\nType errors should appear in your editor as you write code (see setup instructions above), but type checking can also be done through the srb tc command.\nUpdating RBI\nRBI may need to be updated if any of the following happens:\n\nA gem is upgraded or added\nA file is changed that makes use of DSL (eg: a new association defined on a model)\n\nTo update RBI for gems, run the following command:\nbundle exec rake sorbet:update:gems\nTo update all RBI, including RBI for gems, DSL and various other constructs, run the following command:\nbundle exec rake sorbet:update:all\nComing from Typescript?\nSee this handy cheat-sheet.\nResources\nOfficial site\n\n📚 Blogs\n\nShopify: Adopting Sorbet at Scale\nSorbet: Stripe’s type checker for Ruby\n\n\n📹 Talks\n\nRubyConf 2019 Talk: Adopting Sorbet at Scale\n\n\n🏘 Community\n\nSlack\n\n\n\nRelated posts\nLIST WITHOUT ID &quot;[[&quot;+file.name+&quot;]]&quot; + &quot; &quot; + dateformat(date, &quot;yyyy MMM dd&quot;)\nFROM #blog/post\nWHERE contains(file.tags, &quot;static-analysis&quot;) AND file.name != this.file.name"},"Blog/Exploring-User-Sentiment-On-Apps-Using-AI":{"title":"Exploring User Sentiment On Apps Using AI","links":["tags/blog/post"],"tags":["blog/post"],"content":"post\n\nContext\nApp reviews are a great way of understanding how your users feel about your product. It can often highlight issues your users are running into, or how they’re reacting to a new feature you launched.\nThe Problem\nI haven’t seen very many simple, free, easy to use tools to help you understand how users are feeling about your app over time, and how this compares to your competitors. Most tools at least require you to sign up, and many require you to pay for things like sentiment analysis.\nI figured there might be an opportunity for an easy to use (maybe even conversational?) tool that helps app developers understand:\n\nGeneral user sentiment on their app\nCommon positive and negative themes coming up\nHow each of these datapoints are trending over time\nHow their competitors stack up against them\n\nFrom a personal point of view, I’ve been meaning to get more exposure to solving problems using emerging AI technologies and this seemed like a good place to start.\nSolutioning\n1. Web App that calls OpenAI APIs\nAt first I figured this would be a great application for prompt engineering and one of the GPT APIs.\nI started by simply copy pasting a number of reviews from various apps onto ChatGPT and asking it to summarize for me. I tried a few different prompts and both GPT 3.5 &amp; 4. GPT 4 seemed to work significantly better especially when it came to numerical insights, or comparing sentiment over time. One of the prompts that gave me good results was:\n\nYou’re given a list of reviews about an app named ${appName}. Each review will be separated by a blank line. I’d like you to summarize for me the main themes being written about and how many reviews talk about these specific themes. Please group your response into a section about positive sentiment and negative sentiment. Below is the data:\n…\n\nNow I had to figure out a way to do this programmatically. For this I’d need:\n\nAn API to fetch reviews\nA way to call OpenAI\nSome way of presenting the data\n\nApple provides what feels like an old iTunes RSS feed that returns reviews on content. This also seems to work with apps, and appending /json makes it return data in json format. This is accessible at itunes.apple.com/${region}/rss/customerreviews/id=${appId}/sortBy=mostRecent/json.\nCalling OpenAI was pretty simple to do using their official npm package.\nNext.js is something I’ve wanted to explore, so I chose this as the platform to build on.\nI ended up with a web app where you could visit appreviewsai.com/apps/$APP_ID to see a list of user reviews over the past two months (this can be easily extended to support custom date ranges). A lazy loaded component calls the OpenAI gpt-3.5-turbo model to fetch the summary and display it.\nAt this point I ran into a few problems:\n\nThe context window for GPT 3.5 as of now is ~4k tokens. This meant that it would work fine for many apps, but would break for extremely popular apps like TikTok / Instagram, which receive a ton of reviews each day. This should be solvable through sampling.\nThis worked very reliably during local development, but would sometimes fail when deployed to vercel. This turned out to be caused by a 10 second timeout limit they enforce on their free plan. I was able to get past this by simply upgrading to a trial of their pro plan which extends this to 60s.\nFor the vast majority of apps out there, users don’t write new reviews every few seconds. This meant I was calling (and paying for) more OpenAI API calls than I needed to. I solved for this by setting up a supabase database and returning cached data unless there are new reviews since the last call. (Side note: I was very impressed with the Ask Supabase AI feature that allowed me to practically ask it to write the code I needed. This definitely feels like the future of all kinds of documentation!)\n\n2. ChatGPT Plugin\nAt this point I’d accomplished some of what I wanted to do. It did not however support the uase case of comparing sentiment on multiple apps very elegantly, nor did it have an interface for searching for apps. I also didn’t want to spend the time to build things like UI elements to allow for sorting, filtering, changing date ranges etc. A conversational UX would solve all of these problems and more.\nEnter ChatGPT plugins, which I got developer access to very recently. Plugins are a way of extending what ChatGPT can do out of the box. This feature allows you to expose APIs for ChatGPT to intelligently call based on context.\nReading the docs, I realized I could create one relatively easily. There were a handful of components I needed to get this working:\n\nA set of API endpoints for ChatGPT to call. For my application I’d need two main ones:\n\nOne to search for apps given a name\nAnd one to return reviews for a given app ID\n\n\nAn OpenAPI / Swagger spec describing the API endpoints\nA manifest file telling ChatGPT what the plugin can do\n\nI was able to build all of this into the same nextjs app without too much trouble. The main issue I ran into again was the context token limit of ~4k. I was able to get around this by samping reviews so it returns a maximum of 50 regardless of the timeframe requested. This was just a random number I picked that seemed to always work, but this can likely be tuned more preceisely.\nI set up my ChatGPT account to use this plugin and I was pleasantly surprised by just how much it could do! It’s easily able to do things like:\n\nSummarize user reviews and common themes coming up\nAnalyze sentiment over time\nCompare sentiment on your app vs your competitors\nAnswer followup questions\n\nBelow are some screenshots:\n\n\n\nConclusion\nWhile this is a relatively simple example of what ChatGPT plugins can do, what struck me was just how easy it was to set up. Being able to combine the general intelligence of LLMs with the ability to retrieve data or perform actions through API calls, feels incredibly powerful.\nYou can read more about ChatGPT plugins here, sign up for the waitlist here, or learn how to make your own plugin here."},"Blog/Introducing-Titan-Workout-Tracker":{"title":"Introducing Titan Workout Tracker","links":["tags/blog/post","tags/projects/titan"],"tags":["blog/post","projects/titan"],"content":"post titan\n👋 Hi I’m George and I’m the maker of Titan Workout Tracker, a simple to use yet fully featured workout tracker for strength training and body weight based exercises.\nThere are a few different reasons I wanted to build this app:\n\nI found existing workout trackers (I’ve used Strong, Hevy and a few others) to be rather bland (I know, it’s subjective), or lacking in key features. Many of them locked out essential features unless I signed up for an expensive monthly subscription. Strong for example only allows you to have 3 workout plans, and a couple of charts to track your progress on the free version.\nI’ve recently started getting back into working out, and so have a few of my friends. I couldn’t find a nicely built, reasonably priced app that allowed us to follow each other’s progress.\nThere have been a lot of development in AI, including tools that should making it easier to build things. I wanted to see exactly how I could use some of these, and if they live up to the hype.\nI often start many personal projects, but rarely complete them. I wanted to challenge myself to actually see some of them through.\n\nI saw an opportunity here to build a beautifully crafted app to motivate my friends and me to work out more, and hence Titan was born.\nAfter about 2 weeks of writing code on evenings, nights and weekends after work and lots of coffee, v1.0.0 of the app is out on the App Store.\n\nHere are some early screenshots of what I’ve built:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is just the beginning, and I plan to document my experience building this product on this blog.\nWant to try the app out? It’s available here for Android &amp; iOS!"},"Blog/Moving-my-HTTP-website-to-HTTPS-using-LetsEncrypt,-HAProxy-and-Docker":{"title":"Moving my HTTP website to HTTPS using LetsEncrypt, HAProxy and Docker","links":["tags/blog/post"],"tags":["blog/post"],"content":"post\n\n\n                  \n                  Note\n                  \n                \n\nThis is a very old post from 2017. My personal website now just uses github pages for hosting with a custom domain, and handles things like caching, TLS etc.\n\n\nI’ve had a personal website hosted on an AWS EC2 instance with a domain purchased from Namecheap for a while now. My previous setup involved an express server serving a static directory, and running on port 80 on an AWS EC2 instance. I recently changed this up to instead use HTTPS and decided to write a post explaining how you can do so too!\n\nWhat is HTTPS?\nHTTPS builds upon the Hyper Text Transfer Protocol (HTTP) by adding encryption between the web-server and the user’s browser. Chrome, Safari and several other browsers show a lock icon (🔒) in the address bar when viewing a website served over HTTPS.\n\nThere are several advantages to using HTTPS:\n\nEncryption between the browser and the server means a third party in the middle cannot see data being exchanged, ensuring privacy and security.\nHTTPS also protects the integrity of your website by preventing service providers from modifying its contents (ever come across public hotspots that try to inject ads into webpages?).\nHTTPS is the future and is required in order to enable certain modern web browser features such as camera / microphone access, progressive web apps that run offline, service workers etc.\nBetter search engine optimization (SEO). Search engines such as Google rank pages secured over HTTPS higher than pages that are not.\n\nYou can read more about why you should use https at developers.google.com/web/fundamentals/security/encrypt-in-transit/why-https.\nTechnologies used\nTo set this up, I made use of a few technologies:\n\n\nDocker\nDocker is a really cool piece of technology that enables you to create ‘containers’. Containers can contain applications along with anything else it needs to run — an OS, libraries, software dependencies, runtimes etc. Think of it as a very lightweight VM. Since all dependencies for an application are contained within the container, the application is guaranteed to run consistently on any host machine that can run Docker itself. You can read more about docker at their official website — www.docker.com/what-docker.\n\n\n\nHAProxy\nHAProxy, or High Availability Proxy is a really popular load balancer and reverse-proxy application. In our setup, we’ll use this as a layer to proxy all requests received over HTTPS over to our web server serving static files. We’ll also set it up to redirect all HTTP traffic to HTTPS.\n\n\nLet’s Encrypt\nLet’s Encrypt is a free, open Certificate Authority (CA) that can issue SSL certificates. We’ll use this service to generate a free SSL certificate.\n\n\n\nSetup\nThis is what my setup looked like when I started\n\nAn Ubuntu 16.04 AWS EC2 server that I can SSH into.\nA domain I bought from namecheap, configured to point to my AWS server.\nA static express server serving HTML, CSS, JS.\n\nTo start, I went ahead and created two folders on my server to house files for my website and haproxy:\n|-- personal-website\n|-- haproxy\n\nDocker\nI used the official docker install script to set up docker on my remote server. I SSH’ed in and entered the following commands to install docker.\ncurl -sSL get.docker.com/ | sh\nInstructions on how to set up Docker can be found at docs.docker.com/engine/installation/linux/ubuntu/.\nWebsite\nI created the following directory structure in order to keep things tidy. The contents of the website I want published was placed under the publicfolder.\n# website folder structure\n\npersonal-website\n|-- Dockerfile\n|-- package.json\n|-- public/\n    |-- index.html\n\nI used an npm package called http-server to serve my files. To set it up, I simply included it in package.json and rannpm install.\n//package.json\n{\n  &quot;scripts&quot;: {\n    &quot;start&quot;: &quot;./node_modules/.bin/http-server public -p 3000&quot;\n  },\n \n  &quot;dependencies&quot;: {\n    &quot;http-server&quot;: &quot;^0.9.0&quot;\n  }\n}\nRunning npm start and navigating to domain.com:3000 now renders my index.html page.\nThe next step is to tell docker how to build and run our application. I did this by editing the Dockerfile. You can read more about Dockerfiles at docs.docker.com/engine/reference/builder/.\n# Dockerfile\n \n# Tell docker what base image to use\nFROM node:7.5.0-alpine\n \n# Copy package.json over to the docker container and install packages using npm install\n# We copy this here instead of on a later line in order to leverage Docker&#039;s caching.\n# Caching this is beyond the scope of this post, but you can read more bitjudo.com/blog/2014/03/13/building-efficient-dockerfiles-node-dot-js/\nCOPY ./package.json /packages/package.json\n \nRUN cd /packages/ &amp;&amp; npm install -q\n \n# We&#039;ll now use /personal_website as our path to run all subsequent commands\nWORKDIR /personal_website\n \n# Copy over the npm packages (http-server) to this folder\nRUN cp -a /packages/* /personal_website/\n \n# Copy all other contents from the host to the docker container\nCOPY . /personal_website\n \n# Expose the port we&#039;re serving our website on\nEXPOSE 8080\n \n# All steps above this run at build time. The step below tells docker what to do when the container is being run.\nCMD [&quot;node_modules/.bin/http-server&quot;, &quot;public&quot;, &quot;-p 8080&quot;]\nI can now try to build and run this in docker by running the following commands:\n# build the docker image and tag it &#039;personal-website&#039;\ndocker build -t personal-website .\n \n# run the image tagged personal-website we just built, with the container&#039;s port 8080 bound to the host&#039;s port 80\nsudo docker run -it -p 80:8080 personal-website\n🎉 Everything seemed to have worked, and I was able to view my website at [domain.com](domain.com/) (if you’re running this on a local machine and not something like an ec2 instance, you should see it at [http://localhost](http://localhost/)).\nLet’s Encrypt\nThe next step is to request my SSL certificate using Let’s Encrypt. DigitalOcean has a fantastic guide on how to do this, do check it out if this section is not clear. Since I already own a domain, and its A Record is set to my ec2 instance’s DNS, I’m good to proceed. If you haven’t done this already, refer to your domain name provider’s documentation. I SSH’ed into my server and ran the following commands to download the letsencrypt client.\nsudo apt-get update\nsudo apt-get install letsencrypt\nIn order to obtain an SSL certificate, I now need to prove that I own the domain I’m getting the certificate for. letsencrypt lets me do this by automatically placing a secret file in a specified folder which will be served over the internet on my domain.\nThe next part was easier with two terminals open side by side open. On one of them, I ran http-server to serve a folder named ssl.\n# install the http-server npm package on our host machine\nsudo npm install -g http-server\n \n# create a folder for letsencrypt to place a secret file in order to prove ownership of the domain\nmkdir ssl/\n \n# serve this folder on our domain for the letsencrypt server to check ownership\nsudo http-server ssl -p 80\nI used the other terminal to run the letsencrypt client.\n# Replace domain.com with the domain you would like to get the certificate for\nsudo letsencrypt certonly --webroot -w ssl/ -d domain.com\nAt this point I’m asked to provide an email address and to agree to terms and conditions. If everything worked, you’ll be greeted with a message that provides the location of your certificate.\n\n👍 I now have an SSL certificate!\nHAProxy\nThe final piece is to set up HAProxy. To keep things simple, I used the official haproxy docker image to do this.\nI created the following folder structure:\n# HAProxy folder structure\nHAProxy\n|-- Dockerfile\n|-- haproxy.cfg\n|-- private/\n|-- domain.com.pem # place the ssl certificate you obtained here\n\nI updated Dockerfile to the following:\n# Dockerfile\n \n# Use the offical haproxy base image\nFROM haproxy:1.7\n \n# Copy our haproxy configuration into the docker container\nCOPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg\n \n# Copy our ssl certificate into the docker container\nCOPY private/domain.com.pem /private/domain.com.pem\n \n# HAProxy requires a user &amp; group named haproxy in order to run\nRUN groupadd haproxy &amp;&amp; useradd -g haproxy haproxy\n \n# HAProxy also requires /var/lib/haproxy/run/haproxy/ to be created before it&#039;s run\nRUN mkdir -p /var/lib/haproxy/run/haproxy/\nThe haproxy.cfg file looked something like this:\n#haproxy.cfg\nglobal\nchroot /var/lib/haproxy\nuser haproxy\ngroup haproxy\npidfile /var/run/haproxy.pid\n \n# Default SSL material locations\nca-base /usr/local/etc/ssl/certs\ncrt-base /usr/local/etc/ssl/private\n \n# Default ciphers to use on SSL-enabled listening sockets.\nssl-default-bind-options no-sslv3 no-tls-tickets force-tlsv12\nssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS\n \nspread-checks 4\ntune.maxrewrite 1024\ntune.ssl.default-dh-param 2048\ndefaults\nmode http\nbalance roundrobin\noption dontlognull\noption dontlog-normal\noption redispatch\nmaxconn 5000\ntimeout connect 5s\ntimeout client 20s\ntimeout server 20s\ntimeout queue 30s\ntimeout http-request 5s\ntimeout http-keep-alive 15s\nfrontend www-http\nbind 0.0.0.0:80\nreqadd X-Forwarded-Proto:\\ http\ndefault_backend www-backend\nfrontend www-https\n \n#Replace this with the name of your ssl certificate\nbind 0.0.0.0:443 ssl crt /private/domain.com.pem\n \nreqadd X-Forwarded-Proto:\\ https\ndefault_backend www-backend\nbackend www-backend\n \n# Tell haproxy to redirect all http traffic to https\nredirect scheme https if !{ ssl_fc }\nserver www-1 0.0.0.0:8080 check\nI can now test it out in docker by running:\n# Build haproxy container\ndocker build -t haproxy .\n \n# Run the haproxy container\n# We tell the container to use the host&#039;s network adaptor, instead of the default &#039;bridge&#039; mode.\n# Among other things, this enables one container to access ports on another container.\n# Docker networking is out of scope of this post, but you can read more about it at docs.docker.com/engine/userguide/networking/\ndocker run --net=host -it haproxy\nNavigating to domain.com now shows me my website! Trying to access domain.com instead automatically redirects me to domain.com\nFinishing up\nNow that I have everything set up, I can run docker containers on my server in the background using the -d flag. This way our docker containers remain running even after I terminate our SSH session. In order to ensure my application is restarted with the host machine is restarted, or if it crashes due to an error, I’ll also use the --restart flag to specify restart behavior.\n# build and run the personal-website container\ncd personal-website\ndocker build -t personal-website .\ndocker run -d -p 8080:8080 --restart always personal-website\n \n# build and run the haproxy container\ncd ../haproxy/\ndocker build -t haproxy .\ndocker run -d --net=host --restart always haproxy\n🎉 I now have a website served over HTTPS!\nIf you run into issues setting all this up, feel free to leave a comment, or check out my personal-website and haproxy set up on github."},"Blog/My-Obsidian-setup-in-2025":{"title":"My Obsidian setup in 2025","links":["tags/blog/post","Blog/","Blog/2.-Areas/Blog/posts/My-publishing-setup-in-2025/"],"tags":["blog/post"],"content":"post\nPhilosophy\nMy overarching goals with writing notes are 2 fold:\n\nTo help me think clearly\nTo serve as a personal knowledge base I can reference later\n\nOne of the great things about Obsidian is how it’s infinitely customizable. While this is great, I also have the type of personality that finds a lot of joy in the process of hacking and tweaking things, which could get in the way of the goals mentioned above.\nTo keep this in check, I decided to adopt an approach where I keep the tool as vanilla as possible to start, and changing settings and adding plugins only as needed.\nFile organization\nI use the relatively simple PARA system to organize my files. There are probably a few hundred YouTube videos that talk about file organization on Obsidian, and perhaps there’s value in exploring them in the future, but I find that this simple system works well for me for now.\nThe PARA system involves categorizing each file into one of four types:\n\nProject: anything that has an end date (eg: an event I’m planning)\nArea: anything that’ll always be ongoing (eg: hobbies, career etc)\nResource: links to things I didn’t create myself (eg: URLs I want to go back to)\nArchive: any note that’s served its purpose and that I’m unlikely to go back to\n\nIn addition I also have a directory named _Quick Notes  (the _ makes it show up at the top on my file explorer). This is where all new notes I create with CMD + N go to by default before categorization.\nSettings\n\nAll new notes to go to a _Quick Notes directory (Settings -&gt; Files &amp; links -&gt; Folder to create new notes in)\nI use vim keybindings (Settings -&gt; Editor -&gt; Vim keybindings)\nFrontmatter is shown as yaml instead of as a table (Settings -&gt; Properties in Document -&gt; Source)\nAttachments go into an attachments subdirectory in the current directory (Settings -&gt; Files &amp; links -&gt; Default location for new attachments)\nAuto-reveal current file turned on in the file explorer\n\nHotkeys\n\nFIles: Reveal current file in navigation - CMD + shift + R\nNavigate back: Ctrl + o (similar to vim)\nNavigate forward: Ctrl + i (similar to vim)\n\nSyncing and version control\nI use apple devices for the most part, so I mainly use iCloud for syncing. My obsidian vault is in an iCloud directory that gets mirrored to other devices.\nI also back up / version control my vault using git.\n\n\n                  \n                  Warning\n                  \n                \n\nIf you use both a cloud syncing service and git, your .git directory needs to be outside of the directory being synced, or it may result in data loss (I’ve lost some data this way). Use the --git-dir option to set this up.\n\n\nPlugins\nApple Books - Import Highlights\nI do a lot of my reading on my iPad and iPhone. This plugin imports all highlights from all books I have (whether bought from apple or an .epub files opened using the Books app) into markdown notes. Clicking on a highlight even takes you to the specific spot in the book.\n\nBook Search\nThis one lets you make book notes with whatever metadata you specify (eg: title, author, cover image etc). Using the dataview plugin you can query and visualize these notes in whatever way you like, even similar to a completely offline version of GoodReads.\n![[CleanShot 2025-02-21 at 09.04.27@2x.png]]\nCalendar\nShows a calendar view on the sidebar. I can click into a date to quickly start a daily note.\nDataview\nAn incredibly powerful plugin to query all your markdown notes like a database.\nFolder Notes\nMakes it possible to have nested notes like on Notion.\nIconize\nLets you add icons to a lot of places. I just use it for top level directories.\n![[CleanShot 2025-02-21 at 09.13.59@2x.png]]\nJanitor\nLets you find and remove empty files anywhere in your vault\nLazy Plugin Loader\nHaving a lot of plugins can slow down how quickly Obsidian starts, particularly on mobile devices. This plugin lets you defer loading of plugins so they happen in the background after your vault opens. I hope this becomes a built in Obsidian feature.\nNatural Language Dates\nLets you reference dates using commands like @yesterday, @today etc\n\nObsidian Web Clipper\nThis isn’t technically a plugin but a browser extension. It lets you easily create clippings from websites to go back to later.\nPaste URL Into Selection\nMakes it possible to copy a URL from anywhere, highlight some text in obsidian and paste, converting the highlighted text into a link. Similar to how notion works by default.\n\nPeriodic Notes\nEasily create notes for a day, week, month etc. Similar to the Daily Note core plugin . Integrates nicely with the calendar plugin, and lets you click on a date or week to quickly make a periodic note.\nRecent Files\nShows a neat list of files I’ve opened recently on my sidebar.\n![[CleanShot 2025-02-21 at 09.24.41@2x.png]]\nSettings Search\nMakes it possible to search for any setting. This really should’ve been a core built in feature.\nWaypoint\nLet’s you create an automatically updating index page that lists all files in the current directory and nested subdirectories. Works well with the Folder Notes plugin.\n![[CleanShot 2025-02-21 at 09.30.04@2x.png]]\nWebpage HTML Export\nWhat I use to publish this website. More details here - index."},"Blog/My-publishing-setup-in-2025":{"title":"My publishing setup in 2025","links":["tags/blog/post","Blog/Blog"],"tags":["blog/post"],"content":"post\nThroughout the years I’ve tried various setups for publishing blog posts and notes online. I’ve tried custom react sites (using handwritten react apps in the dark days before create-react-app , which itself is now long deprecated) to frameworks like Nextjs and Gatsby. I felt like it never quite hit the spot for me. I found myself spending way more time tweaking the setup than on the actual writing.\nToday my setup is much simpler. I write all my notes in Obsidian. Anything I want to publish online, I do so using a couple of clicks and Nathan George’s excellent Webpage HTML Export plugin.\nThe workflow looks something like this:\n\nWrite content on obsidian\nAdd a post tag\nAdd a publish date to the note’s frontmatter\nThis note and the date metadata gets picked up by a dataview query on the index Blog page\nUse the Webpage HTML export plugin to export the Blog directory into a docs directory in a git repo\nThe git repo on github is set to serve content from this directory via Github Pages on a custom domain\n\nThere are a few things I love about this new setup:\n\nHosting being done on github pages means I don’t need to worry about things like hosting costs, caching, HTTPS certificates or redirecting from HTTP to HTTPS\nObsidian lets me easily link between notes\nThe dataview plugin lets me write simple queries to show dynamic content (for example the Blog page uses a dataview query to show all pages that have the post tag, grouped by year dynamically)\nThe export even shows the graph view from obsidian, showing how notes link to each other\n\nThis is certainly not perfect, but my hope is that with this rather simple system, I can spend less time tweaking the setup, and more time on the content.\nAppendix\nThe markdown code for this page looks like this:\n---\ndate: 2025-02-20\n---\n#blog/post   \n \n&gt; Publish date: `= this.file.frontmatter.date`\n \nThroughout the years I&#039;ve tried various setups for publishing blog posts and notes online. I&#039;ve tried custom react sites (using handwritten react apps in the dark days before [`create-react-app`](github.com/facebook/create-react-app) , which itself is now long deprecated) to frameworks like [Nextjs](nextjs.org/) and [Gatsby](www.gatsbyjs.com/). I felt like it never quite hit the spot for me. I found myself spending way more time tweaking the setup than on the actual writing.\n \nToday my setup is much simpler. I write all my notes in [Obsidian](obsidian.md/). Anything I want to publish online, I do so using a couple of clicks and [Nathan George&#039;s](nathang.dev/) excellent [Webpage HTML Export](github.com/KosmosisDire/obsidian-webpage-export) plugin. \n \nThe workflow looks something like this:\n- Write content on obsidian\n- Add a #blog/post tag\n- Add a publish date to the note&#039;s frontmatter\n- This note and the date metadata gets picked up by a dataview query on the index [[Blog]] page\n- Use the Webpage HTML export plugin to export the Blog directory into a `docs` directory in a git repo\n- The git repo on github is set to serve content from this directory via [Github Pages](pages.github.com/) on a custom domain\n \nThere are a few things I love about this new setup: \n- Hosting being done on github pages means I don&#039;t need to worry about things like hosting costs, caching, HTTPS certificates or redirecting from HTTP to HTTPS\n- Obsidian lets me easily link between notes\n- The [dataview plugin](blacksmithgu.github.io/obsidian-dataview/) lets me write simple queries to show dynamic content (for example the [[Blog]] page uses a dataview query to show all pages that have the #blog/post tag, grouped by year dynamically)\n- The export even shows the graph view from obsidian, showing how notes link to each other\n \nThis is certainly not perfect, but my hope is that with this rather simple system, I can spend less time tweaking the setup, and more time on the content.\n \nThe markdown code for the Blog index page looks like this:\n#blog \n \n\\`\\`\\`dataviewjs\nlet posts = dv.pages(&#039;#blog/post&#039;)\n  .where(p =&gt; p.file.frontmatter.date &amp;&amp; moment(p.file.frontmatter.date, [&quot;YYYY-MM-DD&quot;, moment.ISO_8601], true).isValid())\n  .sort(p =&gt; moment(p.file.frontmatter.date, [&quot;YYYY-MM-DD&quot;, moment.ISO_8601], true), &#039;desc&#039;);\n \nlet groups = {};\nfor (let post of posts) {\n  let m = moment(post.file.frontmatter.date, [&quot;YYYY-MM-DD&quot;, moment.ISO_8601], true);\n  if (!m.isValid()) continue;\n  let year = m.year();\n  if (!groups[year]) groups[year] = [];\n  groups[year].push(post);\n}\n \nlet sortedYears = Object.keys(groups).sort((a, b) =&gt; b - a);\n \nfor (let year of sortedYears) {\n  dv.header(2, year);\n  let items = groups[year].map(post =&gt; {\n    let m = moment(post.file.frontmatter.date, [&quot;YYYY-MM-DD&quot;, moment.ISO_8601], true);\n    let formattedDate = m.format(&quot;MMMM D, YYYY&quot;);\n    return `[[${post.file.name}]] ${formattedDate}`;\n  });\n  dv.list(items);\n}\n\\`\\`\\`"},"Blog/Optimizing-rendering-performance-in-react-native":{"title":"Optimizing rendering performance in react native","links":["tags/blog/post","tags/react","tags/react-native"],"tags":["blog/post","react","react-native"],"content":"post react react-native\nOver the past few weeks, the engineering team at Drop has been doing some really great work on improving app performance. There are several areas that ultimately impact what users perceive as ‘app performance’ that are being worked on. One such area is app rendering - i.e given that data from the API has been fetched, or given that the user has performed an action, how long it takes the app to render (or update) a view.\nIntro to React Native\nAs most of us know, we took an early bet on React Native back when Drop was founded. React native allows us to maintain a single codebase written in typescript that powers both the Android and iOS app.\nThe react native app is composed of a few main parts at a very high level:\n\nComponents\nComponents are units of UI. Every view on the app is created by composing different components (i.e a component can contain other components and so on). Components get their data from something called ‘props’, and update whenever this data changes. Code for a simple component might look something like this:\n\nButton = props =&gt; &lt;Touchable backgroundColor={props.backgroundColor} /&gt;\nComponents re-render anytime data given to them via props change. In this example, the button would re-render if the backgroundColor it’s given changes\n\nRedux state\nThe entire app can be thought of as a giant state machine, where the UI is the output of all the data contained in the redux state at any point in time. Data fetched by the app &amp; actions performed by the user cause the redux state to be updated, which then cause views to update\nEg:\n\n// redux state before making a request to /api/offers\n{\n  entities: {offers: [], categories: []}\n  ids: {offers: [], categories: []}\n}\n// redux state after data from /api/offers is received:\n{\n  entities: {offers: [{id: 1, name: walmart, logo: ...}, {id: 2, name: sephora ...}..], categories: []}\n  ids: {offers: [1, 2, ...], categories: []}\n}\n// after data from /api/categories is received\n{\n  entities: {\n    offers: [{id: 1, name: walmart, logo: ...}, {id: 2, name: sephora ...}..],\n    categories: [{id: 1, name: &#039;Most Popular&#039;}, ...]\n  }\n  ids: {offers: [1, 2, ..], categories: [1, ...]}\n}\n\nSelectors\nThese are functions that read relevant data from the redux state and provide it to components. All selector functions used by components are re-run any time the redux state is updated. Since selectors provide data from the redux store to components, this means that anytime the redux state updates, components are re-rendered, unless memoized correctly. This can lead to what a user might perceive as lag.\n\nEg:\n// Selectors\ngetOffer = (state, id) =&gt; {\n  expensiveComputations()\n  return state.entities.offer[id]\n}\ngetAllOffers = state =&gt;\n  Object.keys(state.ids.offers).map(offerId =&gt; getOffer(state, offerId))\ngetMostPopularOffers = state =&gt; getAllOffers(state).filter(isOfferMostPopular)\nIn the above example, the getMostPopularOffers selector would rerun anytime either state.ids.offers or state.entities.offer updates, even if there are no actual changes to offers to be shown in the Most Popular section.\nProfiling performance\nNow we know what causes components to re-render. But how do we go about finding where this happens, particularly where this happens more than it should? There are a few different tools I’ve found are useful for this:\n1. React Profiler\nThe React Profiler is a development tool that lets you inspect the rendering lifecycles of components. Using it is as simple as running the app in a simulator, enabling debugging, and recording a performance snapshot while interacting with the app.\nStarting up the simulator, and recording a performance profile from app open all the way to when we’re able to dismiss the launchpad, this is what the profiler shows:\n\nThis tells us there are 1330 ‘render cycles’ happening. Render cycles are react’s way of optimizing UI updates - if 20 components need to update within a small time interval, react batches all these up into a single update cycle as opposed to 20 different ones. 1330 cycles here are too many for a human to inspect one by one, and the large number in itself may not indicate issues. To make it easier to find issues, the tool allows us to filter for just render cycles that take more than a given amount of time. Filtering for cycles that take more than an arbitrary &gt; 150ms shows us 4 potentially problematic render cycles.\n\nThe view above shows bars corresponding to the component tree, and where the color indicates rendering time (closer to blue = less time, closer to yellow = more time). Narrowing into the item marked in yellow here tells us a few things:\n\n\nThis component is part of the OfferCategoryListCarousel component, which corresponds to the unbundled categories view on the shop tab.\nThe sidebar on the right tells us that this instance of the component has rendered 14 times during the profiling period, adding up to a total of ~1.5s (Note that the absolute value in development mode here may be different from the experience on a production build, which is a lot more optimized. This is however, directionally informative). We know from the API requests we make that this component should just depend on 2 calls (/offers &amp; /categories) and that a given category view shouldn’t need to re-render that many times. We know also that the shop tab contains about 7 such category carousels using the same logic, so optimizing this could have a sizeable effect.\n\n2. Chrome performance profiler\nThe Chrome performance profiler is yet another, more general purpose devtool that lets you inspect the performance of your javascript app or website. The steps to record a performance snapshot are very similar to the ones used for the react profiler - I.e open chrome, enable debugging in the simulator, hit record, perform actions you want to profile.\nIn contrast to the react profiler that shows individual render cycles, the output of the Chrome profiler is continuous, time based flame graph (i.e render durations correspond to the size of the bars)\n\nHovering on segments in this view shows the amount of time spent on rendering / updating components. We can see for example in the screenshot above that it took 234ms to render the OfferCategoryListCarousel in its current render cycle.\nIt is also possible to zoom into time intervals of interest to see a much more granular view of how much time is spent rendering sub-components. We can for example see here that there is a good chunk of time spent rendering OfferListItem (which correspond to each offer in the unbundled category view)\n\nAnother nice thing about this view is that you can search by component name to find all render / update cycles associated with it.\n\n3. Chrome User timing API\nThe User Timing API allows us to set arbitrary start and stop marks, and inspect time intervals between them with a high level of accuracy (~5 microseconds) . The chrome performance profiling described above technically relies on this API under the hood.\nStart and stop times can be set by calling window.performance.mark(startOrStopTag), and can be sent to the chrome profiling view to be visualized by calling window.performance.measure(&#039;name&#039;, &#039;startTag&#039;, &#039;endTag&#039;). Measured values can also be accessed directly in the console by calling window.performance.getEntriesByType(&#039;measure&#039;)\nAdding this profiling code code to the getDiscoveryOffersByCategory function (this is used to get offer give a category id) for example tells me that it’s called 24 times on app open, and add up to a total of about 56ms.\n\nIt’s also possible to overlay this on a timeline view for easy comparison against time taken for rendering\n\nAdding some profiling code to the getOffer function, I can see that it adds up to about 1.8s, even before any user interaction (note that this is just the time spent on running the selector, not including rendering time). This is a good indicator that this function should be optimized.\n4. console.count\nAnother tool I’ve found useful in debugging performance is the console.count(value) function. This is a really simple function that does 2 things:\n\nPrints the value provided to it\nCounts how many times the provided value is printed\n\nThis function can for example be inserted into a component’s render function to see how many times it updates, or into a selector function to see how many times it gets called with a given argument.\nPlopping this into the getOffer selector function (this is a function that’s used throughout the codebase to look up offers, given offer ids), we see something like:\n\nI.e this function was called 203 different times to look up data for the offer with id 1267. This is a large number of calls to the same function with (mostly) the same arguments, and indicates that this is a really good candidate for memoization.\nFixing rendering performance issues\nWe’ve seen a few ways of identifying performance bottlenecks in react native. Now let’s look at how to fix them.\nThe most common cause of rendering performance issues is improper selector memoization. Memoization is an optimization technique where results of expensive computations are stored, so that subsequent calls results in simple lookups, instead of re-running the expensive computations. This technique trades off higher memory usage for lower processing usage.\n1. Use reselect\nReselect is a library that lets us write memoized compound selectors. Using this makes it so that if selector C is built using selectors A and B, C only re-computes if either A or B recomputes, and not on every state change. Here’s an example:\n// BAD - without memoization\n// This would re-run on every state change and lead to components re-rendering without reason\ngetAllCategories = (state) =&gt; {\n  categoryIds.map = getCategoryIds(state)\n  categoryEntities = getCategoryEntities(state)\n  return categoryIds.map(categoryId =&gt; categoryEntities[categoryId])\n}\n \n// GOOD - with memoization\n// This would only re-run when `state.entities.category` or `state.ids.category` update\ngetAllCategories = createSelector(getCategoryIds, getCategoryEntities, (categoryIds.map, categoryEntities) =&gt; categoryIds.map(categoryId =&gt; categoryEntities[categoryId]))\n2. Do not write selectors that take multiple arguments\nStandard redux selectors should only take a single argument - the redux state. However, there are times when we need to return a value determined by something not available in redux like local component state (eg: whether a checkbox is enabled or disabled). The recommended approach here is to have selectors that return lookup / curried functions\n// BAD - this is an anti-pattern, and is hard to memoize and will re-run unnecessarily\ngetIsFeatureEnabled = (state, flagName) =&gt; {\n  globalFlags = getGlobalFeatureFlags(state)\n  userFlags = getUserFeatureFlags(state)\n  allFlags = { ...globalFlags, ...userFlags }\n  return allFlags[flagName]\n}\n \n// GOOD - return a memoized lookup function\ngetIsFeatureEnabled = createSelector(\n  getGlobalFeatureFlags,\n  getUserFeatureFlags,\n  (globalFlags, userFlags) =&gt; {\n    const allFlags = { ...globalFlags, ...userFlags }\n    return flagName =&gt; allFlags[flagName]\n  }\n)\n3. Use lodash.memoize to deal with selectors that would be too costly to rewrite\nAlthough having selectors that take multiple arguments is an anti-pattern, there are a few of these that are used fairly ubiquitously from a while ago that have accumulated as tech debt. Refactoring away from this is one approach, but this could be costly and lead to new bugs. Instead, we can use loadash.memoize to write our own memoization code to make these perform better.\nmemoize takes 2 arguments - a function to memoize, and a function to compute a cache key to look up past results by. Under the hood, it works by maintaining a data structure that looks something like:\n{ cache-key-1: result1, cache-key-2: result2, ...}\nIf the function to compute the cache key returns the same value as a previous invocation, the expensive function will not be re-run, instead the previous results will be returned. Note that this will only be productive if the function to memoize is more expensive than the function to compute the cache key (since the latter will run on every invocation).\nHere’s an example of how the getOffer selector was memoized (note that separate from this there is also additional logic to handle updating a unique key field on objects every time an API call is made that could affect what the selector should return)\n// BAD\ngetOfferUnmemoized = (state, id) =&gt; {\n  offer = state.entities.offer[id]\n  brand = state.entities.brand[offer.brand]\n  ... doSomeReallyExpensiveStuff()\n  return { ...someComplexObject }\n}\n \n// GOOD - write a wrapper around this to handle memoization\n// This will only rerun if any of the following changes - offer_id, keys corresponding to the offer, its brand or user_offer\ngetOfferMemoized = memoize(\n  getOfferUnmemoized,\n  (state, id) =&gt; {\n    offerEntity = state.entities.offer[id]\n    offerEntityKey = offerEntity.key\n    brandEntity = state.entities.brand[offerEntity.brand]\n    brandEntityKey = brandEntity.key\n    userOfferEntity = getUserOfferByOfferIdLookup(state)(id)\n    userOfferEntityKey = userOfferEntity.key\n    const idKey = id\n    return `${idKey}-${offerEntityKey}-${brandEntityKey}-${userOfferEntityKey}`\n  }\n)\n4. Use shouldComponentUpdate\nReact exposes a lifecycle method called shouldComponentUpdate, which lets us override its default rendering behaviour. Returning true force the component to update, while having it return false makes it not update.\nThis is a powerful tool, but I generally recommend using this only when really needed due to the possibility of introducing inadvertent bugs. Bugs caused by this usually happen when a component grows over time, when someone forgets to update this function when a new prop is added (eg: we’ve seen a bug a long time ago where tapping the ‘add’ button on an offer caused it to not re-render, when it should have updated to the ‘shop’)\n5. Move animations to the native thread\nBy default, animations in react native run on the javascript thread. This is mainly due to historical reasons, where especially on android, some types of animations could not be run on the native thread. However, these days many types of animations are actually supported on the native thread on both iOS &amp; Android. You can see more information here - reactnative.dev/docs/animations\nMaking animations run on the native thread generally leads to better performance, as it frees up the javascript thread for other computations.\nReferences:\n\nUsing the native thread for animations\nReact Native Performance Overview\n\nRelated posts\nLIST WITHOUT ID &quot;[[&quot;+file.name+&quot;]]&quot; + &quot; &quot; + dateformat(date, &quot;yyyy MMM dd&quot;)\nFROM #blog/post\nWHERE contains(file.tags, &quot;react-native&quot;) AND file.name != this.file.name"},"Blog/Picking-a-tech-stack-for-Titan-Workout-Tracker":{"title":"Picking a tech stack for Titan Workout Tracker","links":["tags/blog/post","tags/projects/titan","tags/typescript","tags/react","tags/react-native"],"tags":["blog/post","projects/titan","typescript","react","react-native"],"content":"post titan typescript react react-native\nPicking the right tool for the job is absolutely crucial, especially when you have limited time to spend on it. Picking the wrong stack could lead to tons of wasted effort, something I couldn’t afford on Titan as a one-person project.\nThere are a few main pieces to the stack behind something like Titan. Broadly speaking they are:\n\nFrontend tech\nBackend tech\nDatabase &amp; ORM\nInfrastructure\nMonitoring\n\nI could also opt to go for a more ‘managed’ approach (commonly referred to as ‘serverless’), and not have to worry about infrastructure.\nFrontend\nHigh level my options, to start, were:\n\nNative apps built in kotlin for Android + swift for iOS\nFrameworks that package up web apps to run on mobile (eg: Capacitor or Cordova)\nProgressive Web App (PWA)\nFlutter\nReact native\n\nThe requirements I had for this part of the stack were:\n\nThe end product should be an app that can run on Android and iPhone\n\nPretty much any of the above tech should meet this requirement\n\n\nThe app should look and feel native (i.e I didn’t want it to be a repackaged web app)\n\nThis eliminated the PWA route, and frameworks like Capacitor. It’s hard to get web components to look truly native.\n\n\nI should be able to iterate quickly\n\nThis eliminates building two separate apps in two different technologies (swift / kotlin)\nThis also eliminated Flutter for me, as learning a new language (Dart) would take a little longer. Flutter also has a smaller community than react native, which also means there are fewer third party libraries for it.\nThe App Store and Play Store both have a review process to get updates out. While this exists to ensure quality, it can slow things down quite a bit (particularly on the App Store). The React native ecosystem has tools like Expo Updates and CodePush that enable much quicker Over the Air updates especially for small changes.\n\n\nThe codebase should be maintainable long term\n\nMaintaining 2 codebases with native code would be hard to do\nThis is a personal opinion, but this requirement also eliminated Flutter for me because of its association with Google. I’m personally afraid of building on top of google tech because of their history with breaking changes (eg: angular), and with shutting down projects with little warning (too many examples to list). I wanted to build on a mature technology that had a rock solid community around it.\n\n\n\nThis led me to React Native with typescript. I’ve got a decent bit of experience with this tech, know many of it’s pitfalls and how to avoid them, and it has a large community around it. I used the Expo framework as I didn’t want to reinvent the wheel to solve common problems.\nBackend\nI had some requirements in common here with the frontend, but also some other considerations. My requirements were:\n\nQuick iteration speed &amp; good maintainability\n\n\nI wanted something opinionated and ‘batteries included’, where I don’t have to string together my own framework. I have some experience with Rails, and love these attributes about it. However, the lack of static typing IMO makes it harder to maintain.\nTo an extent I wanted to minimize technology sprawl. It would be a plus if the tech I picked here used the same language as the frontend (i.e typescript).\n\n\nControl &amp; flexibility\n\n\nAn option was to go serverless and build the app using something like Firebase + Cloud Functions or AWS Lambda, but I had concerns about them not being as flexible as I wanted it to be. I didn’t want my business logic to be limited to what the service supports.\nI also didn’t want my code to be vendor locked into a specific technology, one that it might outgrow someday.\nI figured I’d have the most control if I built my own backend with an open source community framework.\n\n\nCost\n\n\nPredictable and low cost was another attribute I was looking for, since I didn’t know whether this project would succeed and bring in enough revenue to cover costs.\nThis meant I probably shouldn’t be setting up a multi-node kubernetes cluster to get started 😂, instead I’d need to start smaller.\n\nIn the end I settled on NestJS. It provides a structured, scalable framework for building server-side applications with TypeScript. It naturally encourages a very modular architecture, supports dependency injection which makes testing easier, and incorporates sane design patterns. It also has a pretty rich ecosystem of libraries.\nIf I were to start the project today, I’d also consider AdonisJS for its similarity to Rails.\nDatabase and ORM\nThe requirements I had for my database were:\n\nHave a strict, consistent schema\nSupport normalized data and efficient, complex queries with JOINs\nSupport transactions (i.e if a part of an operation fails, the part that succeeded is rolled back) and other ACID guarantees\nAllow for the schema to evolve over time, without leading to inconsistent data\nBe scalable\n\nAll this, arguably except (5), meant I’d need a relational database. It would mean that scaling would have to be vertical instead of horizontal, but it would be a fair tradeoff given all the other benefits.\nI started the project with sqlite. However, soon after getting a few real users I moved onto postgres because I was running into bottlenecks with concurrent writes on sqlite.\nI wanted my backend application code that connects to the database to be maintainable (i.e I didn’t want to have raw sql queries). This meant I needed an ORM. Using an ORM gives me a higher level of abstraction to interact with data, and also makes it easier to switch out the underlying database without a lot of code changes.\nThe main contenders here were Sequelize, TypeORM, Prisma and Knex. I picked Prisma for a few reasons:\n\nStrong type safety with typescript\nIntuitive, GraphQL like query structure\nHow your database schema can be defined (and later updated) through an intuitive schema.prisma file. It also has very good IDE support on VSCode.\n\nI’m also a big fan of ActiveRecord on Rails, so I think TypeORM would’ve been nice to work with too.\nInfrastructure\nWhere the code runs\nNow I had to decide where my server and database code would run. There are lots of options here, but they broadly fall into either:\n\nPlatform as a Service or PaaS\n\neg: Heroku, Vercel, Netlify, Elastic Beanstalk\n\n\nInfrastructure as a Service or IaaS\n\neg: Amazon EC2, Google Compute Engine\n\n\n\nI decided to go with Amazon LightSail (it was a slightly cheaper than EC2). For disaster recovery in case something goes wrong with my instance, I also set up daily volume snapshots, which more or less put an upper bound on how much data I could lose.\nContainerization\nTo keep the environment reproducible and straightforward, I containerized the API server and the database using docker.\nService orchestration was done using docker compose.\nTLS / HTTPS\nAPI requests needed to be encrypted using https to prevent man-in-the-middle attacks. This could be done on the API server level, but it’s usually more scalable to do it at a higher level through a reverse proxy. I used traefik to function as my reverse proxy.\nI set up certbot and Let’s Encrypt to generate HTTPS certificates on schedule before they expire. All of this was set up to work on my docker compose setup.\nMonitoring\nI needed a way to know when something went wrong, something other than emails from my users. For this I set up 2 systems:\n\nSentry\n\nSentry alerts me on errors thrown either on the server or on the app\n\n\nNew Relic\n\nNew Relic also picks up on errors, but does a lot more. It can be set up to alert on lots of different scenarios where an error might not be thrown.\nFor example it can ‘call’ me if there’s been an increased rate of HTTP failure codes in a given time period, or if my request times are trending higher than normal.\nI also have it set up on the app, which gives me distributed tracing, where I can track a user interaction to one or more API calls, to specific lines of code that runs on the server. This is a also a great way to debug application performance.\n\n\n\nPersonally I’m also a big fan of Datadog, but it’s more costly for my use case.\nThanks for reading\nIt’s been a lot of fun to build this app. I appreciate you taking the time to read this post. If you’re looking for a workout tracking app that values simplicity &amp; functionality, I invite you to give Titan a try.\nRelated posts\nLIST WITHOUT ID &quot;[[&quot;+file.name+&quot;]]&quot; + &quot; &quot; + dateformat(date, &quot;yyyy MMM dd&quot;)\nFROM #blog/post\nWHERE contains(file.tags, &quot;projects/titan&quot;) AND file.name != this.file.name"},"Blog/The-Philosophy-behind-Titan-Workout-Tracker":{"title":"The Philosophy behind Titan Workout Tracker","links":["tags/blog/post","tags/projects/titan","Blog/introducing-titan","Blog/"],"tags":["blog/post","projects/titan"],"content":"post titan\nA little while ago I wrote briefly about why I built Titan. In this post I’d like to dive a little deeper into the philosophy that’s shaped the app’s development. My journey with fitness and technology has been intertwined for years, and Titan is the culmination of that experience — a tool designed to make workout tracking effortless, enjoyable, and truly user-centric.\nMy Personal Fitness Journey\nI first started lifting weights many years ago while I was in university. Like many students I was juggling academics and a social life and hitting the gym was a way to relieve stress and stay healthy. However, maintaining consistency was a challenge. Since then, I’ve lifted on and off, often with very long periods (1-2+ years) where I’d simply lose interest and stop exercising. My friends often joke about how I’ve been paying for a gym membership since 2011, and how absurd my average cost per workout session probably is.\nThroughout this time, I’ve tried various methods to track my progress and keep myself motivated. I started with Google Sheets but quickly realized how messy things were getting. The manual entry was tedious, and analyzing the data wasn’t as straightforward as I’d hoped. I then moved on to apps, of which there are a ton.\nWhile some of them were decent, they all had limitations that didn’t align with my needs. Many felt overly complex, with unnecessary features cluttering my experience. Others lacked essential functionalities or were too rigid to accommodate my workout style. Some were prohibitively expensive for what they offered, and a significant number didn’t allow for easy data exports. I consider working out a way of life, and I’m not sure I could commit to a single app holding my data for the entirety of my life. Data ownership became a concern — I wanted to ensure I could retain and control my workout history over the long term.\nThe Opportunity\nThis is where I saw an opportunity to build something better than what existed. It wasn’t just about creating another app; it was about crafting a tool that genuinely addressed the pain points I and many others faced. This project also presented a chance for me to flex my product development skills, merging my passion for fitness, technology and the craft of building product.\nI knew that to create an app that stood out, I had to focus on a few main areas:\nSimplicity\nI struggle to name a product I enjoy using that does not embrace simplicity. The best products solve problems without overcomplicating the user experience. For Titan, the user interface had to be as simple and intuitive as possible. It should be obvious how to use the app without a lengthy (or really any) dedicated onboarding flow.\nFrom the moment a user opens Titan, they should feel comfortable navigating through it. This meant eliminating clutter, using clear information architecture, and designing with the user’s natural workflow in mind. There shouldn’t be hard to find menus or convoluted steps. Core actions should be no more than a few taps away.\nFunctionality\nClosely related to simplicity is functionality. Often the two tend to be a balance, but the best products can do both without compromising on either.\nFor me this meant designing an experience where features stay out of the way until needed. A plate calculator for example, is only relevant when entering the weight using an on screen keyboard, and the ‘create a custom exercise’ flow is only relevant when adding an exercise to a workout session. My goal was to make features accessible without being intrusive.\nMeasurement\n\nWhat gets measured gets improved\n\nI’m finding some conflicting info on the origin of this quote, but I think it has a point, particularly when it comes to building habits. Maybe I’m a data nerd, but I didn’t want to shy away from exposing as many metrics as possible, while still keeping the UI simple. To start, Titan has charts showing volume lifted, best set, number of reps etc, broken down by time and by exercise. It also has views showing workout consistency, and I expect that there will be more metrics shown in the future.\nJoy &amp; Satisfaction\nI wanted to also optimize for the emotions felt when using the app. I wanted it to evoke a sense of joy and satisfaction, as opposed to feeling like it was a chore. Working out is hard enough, tracking progress should not add to that.\nI wanted UI elements and flows to “Just Work”, and exactly as one would expect it to (which in software engineering terms is called the Principle of Least Astonishment).\nAnimations and haptics are other great ways to make flows feel satisfying. Animation isn’t an area I have a lot of experience in, but it should be fun to learn. I’ve been really impressed with animations I’ve seen in the Family Wallet app and hope to bring some of my learnings to Titan.\nUI responsiveness is another area that impacts how user experiences are perceived. Interactions should be smooth, and flows should feel immediate and snappy.\nData ownership\nAs mentioned earlier, I think lifting is a way of life, a habit one might have for the entirety of their life. Everything changes, including technology. While I hope I have sucess in building the best product to track workouts right now, I don’t want my users to be chained to this technology forever. To me this means all data on the app should be easily exportable in a universal format (I used json). This should allow users to take their data with them, if / when they leave.\nIt’s about respecting the user’s right to their information and acknowledging that the app serves them—not the other way around.\nThanks for reading\nBuilding Titan has been a labor of love. I appreciate you taking the time to read this post. If you’re looking for a workout tracking app that values simplicity &amp; functionality, I invite you to give Titan a try.\nRelated posts\nLIST WITHOUT ID &quot;[[&quot;+file.name+&quot;]]&quot; + &quot; &quot; + dateformat(date, &quot;yyyy MMM dd&quot;)\nFROM #blog/post\nWHERE contains(file.tags, &quot;projects/titan&quot;) AND file.name != this.file.name"},"Blog/Visualizing-transit-vehicle-locations-on-a-map-in-real-time-using-React":{"title":"Visualizing transit vehicle locations on a map in real-time using React","links":["tags/blog/post"],"tags":["blog/post"],"content":"post\nIn this blog post I’ll talk about how I built a visualization of Toronto Transit vehicle locations and some of the engineering challenges I ran into. A working demo can be found on my personal website (I’ve noticed some corporate networks blocking port 3000 and this application requires it), and all code can be found on github.\n\nPicking Technologies\n1. Tech Stack\nThe first step in building this app was to choose a tech stack to work with. I wanted a stack that would:\n\nLet me view on as many platforms as possible\nNot have to deal with an App Store publish / review process\nI decided to build a web app using React.\n\n2. Map Library\nThere were a couple of different things I wanted from a mapping library:\n\nBe really fast\nWork well with react, or be easy to wrap inside a react component\nBe customizable, in order to prevent it from appearing just like google maps\n\nReact-Map-GL, a library built by Uber seemed to be a great candidate. React-Map-GL is built on top of Mapbox-Gl-Js, which in turn works using WebGL.\n3. Data Source\nMost importantly, I had to find an API that gives me the data I want. I found an API called Nextbus, which provides this data in XML. I then found a wrapper around it called Restbus that provides the same data as JSON. Restbus is provided as an easy to use node library as well as a public url.\n4. Hosting infrastructure\nI decided to use an AWS EC2 instance to host the project.\nBuilding the app\nBackend\nGetting the backend to work seemed really straightforward at first. I could just make requests to the restbus API at restbus.info/api/agencies and display it on a map. However, I ran into some problems with this approach:\nProblem #1: The Restbus API is hosted on HTTP, while my website is hosted on HTTPS. Most browsers block HTTPS websites from requesting insecure resources.\nProblem #2: I noticed that the public Restbus API went down when I made too many consecutive requests. This could be due to some sort of rate limiting.\nI realized I could overcome Problem #1 by deploying my own instance of the Restbus API that calls Nextbus. I could then solve any rate limits imposed by Nextbus by caching responses for a few seconds using redis.\nThis is what my server.js file looked like:\n// server.js\nconst rb = require(&quot;restbus&quot;)\nconst app = require(&quot;express&quot;)()\nconst redis = require(&quot;redis&quot;)\n \nconst redisClient = redis.createClient()\n \n// Get data from redis given a key\nconst getKey = key =&gt;\n  new Promise((resolve, reject) =&gt; {\n    redisClient.get(key, (error, result) =&gt; {\n      if (!error &amp;&amp; result) resolve(result)\n      else reject(error)\n    })\n  })\n \n// Set value for a key in redis with a given expiry\nconst setKey = (key, val, expiry = 5) =&gt; {\n  let str = &quot;&quot;\n  if (typeof val === &quot;string&quot;) str = val\n  else str = JSON.stringify(val)\n  redisClient.setex(key, expiry, str)\n}\n \n// Make the API callable from anywhere\nconst corsMiddleware = (req, res, next) =&gt; {\n  res.header(&quot;Access-Control-Allow-Origin&quot;, &quot;*&quot;)\n  next()\n}\n \napp.use(corsMiddleware)\n \n// Middleware to check if a value was stored in redis\napp.use((req, res, next) =&gt; {\n  getKey(req.url)\n    .then(val =&gt; {\n      console.info(&quot;found in cache&quot;)\n      // Return cached data\n      res.json(JSON.parse(val))\n    })\n    .catch(error =&gt; {\n      console.info(&quot;Not found in cache&quot;)\n      const json = res.json\n      // Override the res.json function to do more than just return json\n      res.json = function (...args) {\n        // Cache the response\n        setKey(req.url, args[1])\n        json.call(this, ...args)\n      }\n      next()\n    })\n})\n \napp.use(&quot;/&quot;, rb.app)\n \napp.listen(3000)\nTo use redis, I simply used the docker image provided by them.\nFrontend\nI used create-react-app to bootstrap the react frontend, and added the react-map-gl library. I ran into some issues with react-map-gl not building correctly (github.com/uber/react-map-gl/issues/176), but fixed it by ejecting from create-react-app, and modifying the webpack config to also process files from react-map-gl.\nI used RxJS to take care of API requests. I could then subscribe to these streams in my map component. I set it up to make requests every 2 seconds.\n// api.js\nimport axios from &quot;axios&quot;\nimport Rx from &quot;rxjs&quot;\n \nconst API_URL = &quot;georgejose.com:3002/agencies/ttc/vehicles&quot;\n \nexport const getVehicles = () =&gt;\n  new Promise((resolve, reject) =&gt; {\n    axios.get(API_URL).then(response =&gt; resolve(response.data))\n  })\n \nexport const timer = Rx.Observable.timer(0, 2000)\n \nexport const $vehicles = timer.flatMap(() =&gt; Rx.Observable.defer(getVehicles))\nThis is what my Map.js component first looked like. Locations are obtained from the $vehicles stream and stored in local component state.\n// map.js\nimport React from &quot;react&quot;\nimport MapGL from &quot;react-map-gl&quot;\n \nimport Marker from &quot;./Marker.js&quot;\n \nimport { $vehicles } from &quot;../utils/api.js&quot;\nimport { getRgbForValue } from &quot;../utils/color.js&quot;\n \nclass InteractiveMap extends React.Component {\n  constructor(props) {\n    super(props)\n    this.state = {\n      viewport: {\n        // Center at Toronto\n        latitude: 43.6536025,\n        longitude: -79.4004877,\n        zoom: 13,\n        width: this.props.width,\n        height: this.props.height,\n        startDragLngLat: null,\n        isDragging: null,\n      },\n      mapStyle: &quot;mapbox://styles/mapbox/dark-v9&quot;,\n      xy: [],\n    }\n  }\n \n  componentDidMount() {\n    // Subscribe to the vehicle location stream\n    $vehicles.subscribe(data =&gt; {\n      this.setState({ xy: data })\n    })\n  }\n \n  componentWillReceiveProps(nextProps) {\n    // Resize map if window is resized\n    if (nextProps.height) {\n      const newState = this.state\n      newState.viewport.height = nextProps.height\n      this.setState(newState)\n    }\n    if (nextProps.width) {\n      const newState = this.state\n      newState.viewport.width = nextProps.width\n      this.setState(newState)\n    }\n  }\n \n  _onChangeViewport = newViewport =&gt; {\n    const viewport = Object.assign({}, this.state.viewport, newViewport)\n    this.setState({ viewport })\n  }\n \n  render() {\n    const { mapStyle, viewport } = this.state\n    return (\n      &lt;MapGL\n        mapboxApiAccessToken=&quot;XXX&quot;\n        onChangeViewport={this._onChangeViewport}\n        mapStyle={mapStyle}\n        ref={map =&gt; (this.map = map)}\n        {...viewport}\n      &gt;\n        {this.state.xy.map((xy, i) =&gt; {\n          return (\n            &lt;Marker\n              xy={{ x: xy.lat, y: xy.lon }}\n              color={getRgbForValue(xy.secsSinceReport)}\n              key={i}\n              text={xy.routeId}\n            /&gt;\n          )\n        })}\n      &lt;/MapGL&gt;\n    )\n  }\n}\n \nexport default InteractiveMap\nPerformance Engineering\n\nOne of the big issues I ran into was lag. Lots of it while panning the map around. Running the chrome profiler showed me the following:\n\nThe component update was taking 359ms to update after each API request. I soon noticed that this is caused due to the fact that all ~900 markers were updating on every request, regardless of what was displayed within the window.\nAdding a condition to only update markers that are within the current window boundaries and re-running the chrome profiler showed me the following:\n\n47.27ms instead of 359.84ms for the same zoom level, an improvement of ~750%!\n\nI also noticed that network requests take rather long.\n\n\nLet’s see if the restbus API returns any data we do not need.\n{\n  &quot;id&quot;: &quot;8084&quot;,\n  &quot;routeId&quot;: &quot;168&quot;,\n  &quot;directionId&quot;: &quot;168_1_168&quot;,\n  &quot;predictable&quot;: true,\n  &quot;secsSinceReport&quot;: 14,\n  &quot;kph&quot;: null,\n  &quot;heading&quot;: 266,\n  &quot;lat&quot;: 43.682335,\n  &quot;lon&quot;: -79.469849,\n  &quot;leadingVehicleId&quot;: null,\n  &quot;_links&quot;: {\n    &quot;self&quot;: {\n      &quot;href&quot;: &quot;georgejose.com:3002/agencies/ttc/vehicles/8084&quot;,\n      &quot;type&quot;: &quot;application/json&quot;,\n      &quot;rel&quot;: &quot;self&quot;,\n      &quot;rt&quot;: &quot;vehicle&quot;,\n      &quot;title&quot;: &quot;Transit agency ttc vehicle 8084.&quot;\n    },\n    &quot;to&quot;: [\n      {\n        &quot;href&quot;: &quot;georgejose.com:3002/agencies/ttc/vehicles/8084&quot;,\n        &quot;type&quot;: &quot;application/json&quot;,\n        &quot;rel&quot;: &quot;self&quot;,\n        &quot;rt&quot;: &quot;vehicle&quot;,\n        &quot;title&quot;: &quot;Transit agency ttc vehicle 8084.&quot;\n      }\n    ],\n    &quot;from&quot;: [\n      {\n        &quot;href&quot;: &quot;georgejose.com:3002/agencies/ttc/vehicles&quot;,\n        &quot;type&quot;: &quot;application/json&quot;,\n        &quot;rel&quot;: &quot;section&quot;,\n        &quot;rt&quot;: &quot;vehicle&quot;,\n        &quot;title&quot;: &quot;A collection of vehicles for agency ttc.&quot;\n      },\n      ...\n    ]\n  }\n}\nWe’re only using a few fields out of this — id, routeId, secsSinceLastReport, lat, lon, and everything else can be ignored. Filtering this on the backend using an express middleware before sending a json response gives us:\n\nResponse times are on average a lot lower!\n\nFunctional Components\n\nThe React docs recommend writing components as pure stateless functions whenever possible. Future improvements to react will have performance optimizations to components declared this way by avoiding unnecessary checks, memory allocations and lifecycle methods.\nA hack to make functional components faster right now is to call them as functions, as opposed to JSX nodes.\nconst Component = (props) =&gt; (\n \n  &lt;div&gt;{props}&lt;/div&gt;\n);\n// Using component in JSX =&gt; slower\n&lt;div&gt;\n  &lt;Component /&gt;\n&lt;/div&gt;\n// Calling component as a regular function in curly braces =&gt; faster\n&lt;div&gt;\n  {Component()}\n&lt;/div&gt;\nAlthough I didn’t see a huge impact from this personally, there are several blog posts that report a big speedup. Here’s one:\n\n45% Faster React Functional Components, Now\n\n\nFurther Reading:\n\nReact Performance Tools\nReact-Map-Gl\n"},"Blog/Writing-better-code-using-static-analysis":{"title":"Writing better code using static analysis","links":["tags/blog/post","tags/ruby","tags/ruby-on-rails","tags/typescript","tags/static-analysis"],"tags":["blog/post","ruby","ruby-on-rails","typescript","static-analysis"],"content":"post ruby ruby-on-rails typescript static-analysis\nStatic code analysis is the concept of examining source code and predicting how it will behave without actually running it. Often it’s used by automated tooling as part of the development process to prevent problematic code from making it into a mainline branch. It is designed to complement other processes like manual testing, automated tests and pull request reviews.\nStatic code analysis is something we use quite heavily at Drop, particularly on the frontend, but also on the backend. Broadly speaking, the static analysis tools we use fall under two categories:\n\nType-checking (which we won’t really get into in this post)\nLinting\n\nA core concept in static code analysis is the Abstract Syntax Tree (AST). An AST is a tree representation of source code, where each node represents a construct in the source code. The word ‘abstract’ is used because it does not represent every detail in the syntax, just detail relating to the structure &amp; content of the source code. You can technically have different source code that maps to the same AST (for example you can format your code differently and end up with the same AST).\nAST corresponding to the ruby code below:\ndef add(a, b)\n  sum = a + b\n\treturn sum\nend\nmay look something like this:\n\nThis can also be expressed as an S-expression:\n(def :add\n  (args\n    (arg :a)\n    (arg :b))\n  (begin\n    (lvasgn :sum\n      (send\n        (lvar :a) :+\n        (lvar :b)))\n    (return\n      (lvar :sum))))\n\nLinting is usually done by inspecting the AST and flagging forbidden constructs and usage. At Drop, we use 2 main tools for this:\n\nEslint for Typescript\nRuboCop for Ruby\n\nThe typescript, react, ruby &amp; rails communities all have rich selections of pre-written lint rules available that we use. But as we scale our team, it becomes helpful to write our own rules to enforce our internal coding standards. Enforcing coding conventions as rules that can be validated systematically and automatically has the following benefits:\n\nSome types of work are just better suited to machines. Enforcing a lint rule through automation is a lot more reliable than having engineers manually identify and flag it in PR reviews.\nIt reduces cognitive overhead for the engineer writing code. They can now focus more on writing good business logic, and less on whether they’re violating an internal coding convention\nIt allows us to keep a high bar for code consistency and maintainability\nIt scales easily as we onboard new engineers. The effort required to write a rule is the same, whether we have 5 engineers contributing to the codebase, or 500.\n\nAnalyzing ASTs and writing custom lint rules sounds scary, but it really isn’t with the right tooling and with some understanding of the fundamentals. Let’s look at a few custom rules we’ve adopted at Drop and how we went about it:\nRuby\nRubocop/mdc\nThis is a custom rule we wrote to identify issues with how we use Mapped Diagnostic Context (MDC). MDC is something we use on the backend to keep track of basic diagnostic information through call stacks (information like user_id for example). Errors and log lines should contain this object of key/value pairs to assist with debugging.\nTo use MDC correctly, each method that adds to this object needs to push and pop frames in a consistent way. push in the context of a method, without a corresponding pop at the end, or vice versa will lead to very hard to reason about issues with our logs. These types of issues will not be apparent until when we need to debug a production issue, and end up unable to find the relevant logs as they’re not tagged correctly.\nBelow are some examples of problematic code:\n# Has a mdc.push but does not have an mdc.pop at the end\ndef method\n\tmdc.push({})\n\tmdc[:user_id] = 123\n\t# do stuff\nend\n \n# Does not have an mdc.push at the beginning\ndef method\n\t# do stuff\n\tmdc.pop\nend\n \n# Only calls mdc.pop sometimes\ndef method\n\tmdc.push({})\n\tmdc[:user_id] = 123\n\t# do stuff\n\tmdc.pop if some_condition\nend\nThis is an example of correct code:\ndef method\n\tmdc.push({})\n\tmdc[:user_id] = 123\n\t# do stuff\nensure\n\tmdc.pop\nend\nSimply doing a text search on the backend repo for mdc.push gives us 190 results. Going through these one by one and inspecting for problematic usage would not be very fun nor reliable. It also would not prevent another engineer who isn’t aware of this from adding problematic code in the future.\nThis is a great use case for an automated lint rule!\nA great way to start is by going to ruby-ast-explorer.herokuapp.com and inspecting S-expressions corresponding to some good and some bad code.\nInspecting the S-expression for some correct code looks like this:\n(def :method\n  (args)\n  (ensure\n    (begin\n      (send\n        (send nil :mdc) :push\n        (hash))\n      (send\n        (send nil :mdc) :[]=\n        (sym :user_id)\n        (int 123)))\n    (send\n      (send nil :mdc) :pop)))\nA rule to enforce that all methods that have a mdc.push has a corresponding mdc.pop can be implemented like so:\nclass EnsureMdcCop &lt; Cop\n  MSG = &#039;Make sure you have a corresponding mdc.pop. Link to best practices...&#039;\n  def_node_matcher :mdc_push?, &lt;&lt;~PATTERN\n    (send (send (...) :mdc) :push ...)\n  PATTERN\n \n  def on_send(node)\n    return unless mdc_push?(node)\n    ancestor_ensure_blocks = node.ancestors.select do |ancestor|\n      ancestor.type == :ensure\n    end\n    ancestor_ensure_block = ancestor_ensure_blocks.first\n \n    unless ancestor_ensure_block # Check if mdc operations are done inside an ensure block\n      add_offense(node)\n      return\n    end\n \n    has_mdc_pop = ancestor_ensure_block.children.any do |child| # check that there is a corresponding mdc.pop\n      has_only_mdc_pop_in_ensure = NodePattern.new(&#039;(send (send (...) :mdc) :pop ...)&#039;).match(child)\n      has_mdc_pop_in_ensure_with_other_lines = child.each_descendant.any do |descendant|\n        NodePattern.new(&#039;(send (send (...) :mdc) :pop ...)&#039;).match(descendant)\n      end\n      has_only_mdc_pop_in_ensure || has_mdc_pop_in_ensure_with_other_lines\n    end\n    return if has_mdc_pop\n    add_offense(node)\n  end\nend\nThis now flags any problematic code right within one’s editor &amp; as part of Continuous Integration checks.\n\nTypescript\nImplementing lint rules for Typescript through eslint is just as easy as doing so for Ruby through rubocop. Let’s look at one.\n@drop-engineering/eslint-plugin/type-yield-expressions\nWe make extensive use of redux-saga within the app. Redux-saga is a library to manage what are called ‘side effects’ in the redux world. A ‘side-effect’ is a response to a redux action that isn’t just a redux state update. For example: fetching data in response to a redux action.\nRedux-saga works using generator functions. Generator functions by nature cannot be typed automatically, as in theory the same generator can yield values of different types.\nfunction* generatorYieldingDifferentTypes() {\n  yield 1\n  yield &quot;string&quot;\n}\n \nconst gen = generatorYieldingDifferentTypes()\nyield1 = gen.next() // this is a number\nyield2 = gen.next() // this is a string\nSince sagas rely on generators, and limitations to inferring types automatically, it can in practice lead to some dangerous code. Here’s an example:\nfunction* handleOfferActivationSaga() {\n  const action = yield take(&quot;OFFER_ACTIVATE_REQUEST&quot;) // &#039;action&#039; here is implicitly typed to &#039;any&#039;\n  const offer = yield select(getOffer, action.payload.id) // This action actually has an &#039;offer_id&#039; parameter, not &#039;id&#039;. This issue will unfortunately not be detected unless this specific case is tested manually, or someone notices in a PR\n}\nWe can avoid this issue by adding explicit type annotations to yield statements\nconst action: ReturnType&lt;typeof activateOffer&gt; = yield take(\n  &quot;OFFER_ACTIVATE_REQUEST&quot;\n)\nconst offer: ReturnType&lt;typeof getOffer&gt; = yield select(\n  getOffer,\n  action.payload.id\n) // This is now flagged\n\nIt turns out we can very easily encode a rule to ensure that all yield expressions have explicit typings. We start by inspecting the AST for some correct and incorrect code examples. astexplorer.net/ is a great tool for this (make sure you select the same parser we use in our system - @typescript-eslint/parser).\nThe AST for problematic code:\nfunction* incorrectCode() {\n  const value = yield select(selector)\n}\nlooks something like this:\n{\n  type: &#039;FunctionDeclaration&#039;,\n  body: {\n    type: &#039;BlockStatement&#039;,\n    body: [\n      {\n        declarations: [\n          {\n            type: &#039;VariableDeclarator&#039;,\n            id: { type: &#039;Identifier&#039; },\n            init: { type: &#039;YieldExpression&#039; },\n          },\n        ],\n      },\n    ],\n  },\n}\nWhile the AST for correct code:\nfunction* incorrectCode() {\n  const value: Type = yield select(selector)\n}\nlooks something like this:\n{\n  type: &#039;FunctionDeclaration&#039;,\n  body: {\n    type: &#039;BlockStatement&#039;,\n    body: [\n      {\n        type: &#039;VariableDeclaration&#039;,\n        declarations: [\n          {\n            type: &#039;VariableDeclarator&#039;,\n            id: {\n              type: &#039;Identifier&#039;,\n              typeAnnotation: { type: &#039;TSTypeAnnotation&#039; }, // This line is the main difference\n            },\n            init: { type: &#039;YieldExpression&#039; },\n          },\n        ],\n      },\n    ],\n  },\n}\nUsing this information, we can implement a custom lint rule to enforce that all yield expressions have an explicit typing. Below is code that does this:\nmodule.exports = {\n  meta: {\n    docs: { description: &quot;Outputs of yield expressions should be typed&quot; },\n  },\n  create: context =&gt; {\n    return {\n      VariableDeclaration(node) {\n        const isYieldExpression =\n          get(node, &quot;declarations.0.init.type&quot;) === &quot;YieldExpression&quot;\n        const hasTypeAnnotation =\n          get(node, &quot;declarations.0.id.typeAnnotation.type&quot;) ===\n          &quot;TSTypeAnnotation&quot;\n \n        if (isYieldExpression &amp;&amp; !hasTypeAnnotation)\n          context.report({\n            node,\n            message: &quot;Outputs of yield expressions should be typed&quot;,\n          })\n      },\n    }\n  },\n}\n\nTLDR: Enforcing internal coding conventions through custom static checks is generally not as hard as it sounds. Doing so enables us to write more maintainable, higher quality code that automatically scales with the team.\nReferences:\n\nAbstract Syntax Trees\nStatic code analysis\nAST explorer for many languages\nRubocop: Custom Cops for Custom Needs\n\nRelated posts\nLIST WITHOUT ID &quot;[[&quot;+file.name+&quot;]]&quot; + &quot; &quot; + dateformat(date, &quot;yyyy MMM dd&quot;)\nFROM #blog/post\nWHERE contains(file.tags, &quot;static-analysis&quot;) AND file.name != this.file.name"},"Blog/attachments/attachments":{"title":"attachments","links":[],"tags":[],"content":""},"Contact":{"title":"Contact","links":[],"tags":[],"content":"\nWebsite\nEmail\nBlueSky\nX\nLinkedin\n"},"Resume":{"title":"Resume","links":["2.-Areas/Blog/posts/Optimizing-rendering-performance-in-react-native/","2.-Areas/Blog/posts/Exploring-Sorbet-with-Rails-at-Drop/","2.-Areas/Blog/posts/Introducing-Titan-Workout-Tracker/"],"tags":[],"content":"Summary\n\nProduct-minded engineering generalist with years of experience in various capacities – technical leadership, people, process &amp; project management, hands-on software development\nExtensive experience leading teams and executing complex, open-ended projects in startup and corporate environments\n\nWork Experience\nDrop Technologies Inc\nSenior Engineering Manager\n’22 - present\n\nManaged 2 high performing, full-stack product engineering teams (~15 individuals) and their roadmaps\nTeam 1 responsible for Growth and New User Activation\nTeam 2 responsible for a brand new Market Research product allowing brands to run custom surveys granularly targeted to users by spend\nLed building a stock marketing investing product based on user spend data\n\nEngineering Manager\nCore Team, ‘19 – ‘22\n\nManaged a full stack product engineering team responsible for the mobile app’s core user experience; successfully scaled it into 2 distinct teams, each with their own areas of focus\nWorked with company leadership on translating company vision into objectives and key results\nWorked closely with product managers, engineers, data scientists, designers and cross functional business teams on identifying, prioritizing and executing impactful projects\nProvided technical leadership to the wider Drop engineering team, led several high impact, cross-team technical efforts\nNotable projects\n\nApp Performance optimizations: Led a project that resulted in drastic improvements to app responsiveness on the iOS &amp; Android (react native) apps. See also index.\nCodePull: Built an internal system that allows anyone at Drop to load up app changes from a pull request on their device by simply clicking an automatically generated link\nTest tooling: Built an internal framework that enables engineers to easily write comprehensive app integration tests leveraging realistic mock data\nStatic type checking in Ruby: Spearheaded the adoption of Sorbet on the backend codebase for improved developer experience, productivity and code quality. See also index.\n\n\n\nEngineering Lead\nGrowth Team, ‘18 – ‘19\n\nLed a full stack engineering team of 4 individuals focused on product goals including user acquisition, engagement and retention\nLed tooling, process and other infrastructure initiatives focused on increasing developer happiness, productivity and code quality\nNotable projects\n\nTypescript: made the case for, got buy-in and spearheaded migration; built internal tooling to track its adoption and assist with incremental migration; defined best practices and internal constructs to effectively write typesafe code\nContinuous Deployment: built tooling to continuously deploy merged app code to a staging environment\nStatic code analysis: built internal rules for eslint &amp; rubocop; rules rely on AST analysis to flag certain types of problematic code\n\n\n\nSoftware Engineer\nDrop Engineering Team, ‘18\n\nWorked as one of the main primarily frontend-focused, full-stack software engineers building the Drop mobile app in react native + backend in ruby on rails\nBuilt and owned several impactful product domains including in-app shopping, referral program, card linking, marketing comms infrastructure, search, rewards etc\nFormalized and drove adoption of a regular app release process; also built tooling to enable anyone to quickly and safely release code\n\nDeloitte Canada\nSenior Consultant ‘17 - ‘18\n\nTech Lead, Ecommerce Security Enhancement Program – Leading Canadian Retailer\n\nLed a project to enhance security across 4 ecommerce stores owned by the retailer\n\n\nTech Lead, Self Scan &amp; Checkout Program – Loblaw Digital\n\nLed a project to build an in-store mobile scan and checkout experience\nLed building a user facing Point of Sale system for checkouts as well as the APIs integrating with the backend system\n\n\n\nConsultant ‘16 - ‘17\n\nSolutions Engineer, PC Optimum\n\nFrontend solution engineer on a net new web presence for Canada’s largest loyalty program\n\n\n\nBusiness Technology Analyst ‘15 - ‘16\n\nSolutions Architect, Sonnet Insurance - Digital Transformation Program\n\nWorked on solutions architecture team on a net new, multi-phased Direct to Consumer insurance program\nWorked across technical teams to ensure systems being built were inter- operable, followed development best practices and met security &amp; performance requirements\n\n\n\nPersonal Projects\n\nTitan Workout Tracker\n\nA beautifully crafted, cross platform app to track weightlifting exercises &amp; fitness metrics, solo, or with friends\nRead more at index\n\n\nA Journal A Day\n\nA privacy focused, completely local, cross platform app for daily, secure, journal entries\n\n\nCommunauto Watcher\n\nA react native app that allows users to set a geographic area to watch for and get notified when cars from a car share service are available to rent\nPowered by a backend server running in koa &amp; prisma ORM for postgres\n\n\n"},"content":{"title":"Hi I'm George","links":["Blog/","Resume","Contact"],"tags":[],"content":"\nBlog\nResume\nContact\n"},"index":{"title":"Hi I'm George","links":["Blog/","Resume","Contact"],"tags":[],"content":"\nBlog\nResume\nContact\n"}}